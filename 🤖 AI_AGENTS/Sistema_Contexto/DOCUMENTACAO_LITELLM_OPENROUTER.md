# Documenta√ß√£o LiteLLM com OpenRouter

## Vis√£o Geral

Este projeto demonstra como usar o LiteLLM como proxy para a API do OpenRouter, permitindo cache local e gerenciamento de contexto expandido para modelos de IA.

## Configura√ß√£o Inicial

### 1. Instala√ß√£o de Depend√™ncias

```bash
pip install -r requirements.txt
```

### 2. Configura√ß√£o de Vari√°veis de Ambiente

Copie o arquivo `.env.example` para `.env` e configure sua chave da API do OpenRouter:

```bash
cp .env.example .env
```

Edite o arquivo `.env`:
```
OPENROUTER_API_KEY=sk-or-v1-sua-chave-aqui
LITELLM_LOG_LEVEL=INFO
LITELLM_CACHE_TYPE=disk
LITELLM_CACHE_DIR=./cache/litellm_cache
```

### 3. Configura√ß√£o do LiteLLM

O arquivo `litellm_simple.yaml` cont√©m a configura√ß√£o do proxy:

```yaml
model_list:
  - model_name: deepseek-r1-free
    litellm_params:
      model: openrouter/deepseek/deepseek-r1-0528:free
      api_key: sk-or-v1-sua-chave-aqui
      api_base: https://openrouter.ai/api/v1

general_settings:
  disable_auth: true
  cache:
    type: disk
    disk_cache_dir: ./cache/litellm_cache
  master_key: null
  database_url: null
```

## Uso do Sistema

### 1. Iniciando o Proxy LiteLLM

```bash
litellm --config litellm_simple.yaml --port 4000 --host 0.0.0.0
```

Ou use o script Python:

```bash
python start_proxy.py
```

### 2. Testando a Conex√£o

Use o script de teste para verificar se tudo est√° funcionando:

```bash
python test_simple_proxy.py
```

### 3. Exemplo de Uso em Python

```python
import openai

# Configure o cliente para usar o proxy local
client = openai.OpenAI(
    base_url="http://localhost:4000",
    api_key="fake-key"  # N√£o necess√°rio com disable_auth: true
)

# Fa√ßa uma requisi√ß√£o
response = client.chat.completions.create(
    model="deepseek-r1-free",
    messages=[
        {"role": "user", "content": "Ol√°! Como voc√™ est√°?"}
    ],
    max_tokens=150
)

print(response.choices[0].message.content)
```

## Estrat√©gias para Contexto Expandido

### 1. Chunking Inteligente

Use o script `exemplo_chunking_inteligente.py` para processar documentos grandes:

```bash
python exemplo_chunking_inteligente.py
```

### 2. Cache de Contexto

O LiteLLM automaticamente faz cache das respostas no diret√≥rio `./cache/litellm_cache/`.

### 3. T√©cnicas Avan√ßadas

Consulte o arquivo `GUIA_AUMENTAR_CONTEXTO_LOCAL.md` para estrat√©gias avan√ßadas:

- Chunking com sobreposi√ß√£o
- Cache de contexto com embeddings
- Sumariza√ß√£o autom√°tica
- Compress√£o de contexto
- Uso de modelos locais (Ollama, LM Studio)

## Modelos Dispon√≠veis

### OpenRouter (Gratuitos)

- `deepseek/deepseek-r1-0528:free` - Modelo DeepSeek R1 gratuito
- `openai/gpt-3.5-turbo:free` - GPT-3.5 Turbo gratuito (limitado)

### Configura√ß√£o de Novos Modelos

Para adicionar novos modelos, edite o arquivo `litellm_simple.yaml`:

```yaml
model_list:
  - model_name: novo-modelo
    litellm_params:
      model: openrouter/provider/model-name
      api_key: ${OPENROUTER_API_KEY}
      api_base: https://openrouter.ai/api/v1
```

## Troubleshooting

### Erro 401 (N√£o Autorizado)

1. Verifique se a chave da API est√° correta no arquivo `.env`
2. Confirme se `disable_auth: true` est√° no `litellm_simple.yaml`
3. Reinicie o proxy ap√≥s mudan√ßas na configura√ß√£o

### Erro 429 (Limite de Taxa)

1. Aguarde alguns minutos antes de tentar novamente
2. Considere usar modelos pagos para limites maiores
3. Implemente retry logic com backoff exponencial

### Cache N√£o Funcionando

1. Verifique se o diret√≥rio `./cache/litellm_cache/` existe
2. Confirme as permiss√µes de escrita no diret√≥rio
3. Verifique os logs do LiteLLM para erros

### Problemas de Contexto

1. Use chunking para textos muito grandes
2. Implemente sumariza√ß√£o para manter contexto relevante
3. Considere modelos locais para contextos muito grandes

## Scripts √öteis

- `test_simple_proxy.py` - Teste b√°sico do proxy
- `test_direct_openrouter.py` - Teste direto da API OpenRouter
- `exemplo_chunking_inteligente.py` - Demonstra√ß√£o de chunking
- `start_proxy.py` - Script para iniciar o proxy

## Logs e Monitoramento

Os logs do LiteLLM s√£o exibidos no terminal. Para logs mais detalhados:

```bash
export LITELLM_LOG_LEVEL=DEBUG
litellm --config litellm_simple.yaml --port 4000 --host 0.0.0.0
```

## Considera√ß√µes de Performance

1. **Cache**: Ative o cache em disco para melhor performance
2. **Chunking**: Use chunks de 2000-4000 tokens para melhor efici√™ncia
3. **Modelos Locais**: Para uso intensivo, considere Ollama ou LM Studio
4. **Rate Limiting**: Implemente delays entre requisi√ß√µes para evitar 429

## üöÄ Sistema de Contexto Expandido (2M Tokens)

### Vis√£o Geral

O sistema de contexto expandido permite processar documentos de at√© **2 milh√µes de tokens**, superando o limite de 163k tokens do OpenRouter atrav√©s de t√©cnicas avan√ßadas:

- **Chunking Hier√°rquico Inteligente**: Divis√£o inteligente do texto preservando contexto
- **Cache de Contexto com Embeddings**: Busca sem√¢ntica por relev√¢ncia
- **Sumariza√ß√£o Autom√°tica Progressiva**: Compress√£o din√¢mica de contexto
- **Processamento Paralelo**: M√∫ltiplos chunks processados simultaneamente

### Instala√ß√£o R√°pida

```bash
# 1. Instalar depend√™ncias automaticamente
python instalar_sistema_contexto.py

# 2. Configurar chave de API no .env
echo "OPENROUTER_API_KEY=sua-chave-aqui" >> .env

# 3. Testar o sistema
python exemplo_uso_contexto_2m.py
```

### Arquivos do Sistema

| Arquivo | Descri√ß√£o |
|---------|----------|
| `sistema_contexto_expandido_2m.py` | Sistema principal de contexto expandido |
| `exemplo_uso_contexto_2m.py` | Exemplo pr√°tico de uso com 2M tokens |
| `instalar_sistema_contexto.py` | Instalador autom√°tico de depend√™ncias |
| `GUIA_AUMENTAR_CONTEXTO_LOCAL.md` | Guia detalhado de estrat√©gias |

### Exemplo de Uso

```python
from sistema_contexto_expandido_2m import SistemaContextoExpandido

# Inicializar sistema
sistema = SistemaContextoExpandido(
    api_key=os.getenv('OPENROUTER_API_KEY'),
    modelo_principal='deepseek/deepseek-r1-0528:free',
    limite_tokens_modelo=150000,
    cache_dir='./cache_contexto_2m'
)

# Processar documento grande
resposta = sistema.processar_contexto_expandido(
    texto=documento_2m_tokens,
    pergunta="Analise este documento e extraia os pontos principais",
    max_tokens_resposta=2000
)
```

### Capacidades do Sistema

- ‚úÖ **Processamento de 2M+ tokens** em documentos √∫nicos
- ‚úÖ **Cache inteligente** com embeddings sem√¢nticos
- ‚úÖ **Busca por relev√¢ncia** nos chunks mais importantes
- ‚úÖ **Sumariza√ß√£o progressiva** para manter contexto
- ‚úÖ **Processamento paralelo** para melhor performance
- ‚úÖ **Monitoramento de custos** e uso de tokens

### Performance Esperada

| Tamanho do Documento | Tempo de Processamento | Tokens/Segundo |
|---------------------|----------------------|----------------|
| 500k tokens | ~2-3 minutos | ~3,000 |
| 1M tokens | ~4-6 minutos | ~3,500 |
| 2M tokens | ~8-12 minutos | ~4,000 |

### Configura√ß√µes Avan√ßadas

```python
# Configura√ß√£o personalizada
sistema = SistemaContextoExpandido(
    api_key="sua-chave",
    modelo_principal="deepseek/deepseek-r1-0528:free",
    limite_tokens_modelo=150000,
    tamanho_chunk=8000,          # Tamanho dos chunks
    sobreposicao_chunk=500,      # Sobreposi√ß√£o entre chunks
    max_chunks_paralelos=3,      # Processamento paralelo
    threshold_relevancia=0.7,    # Limite de relev√¢ncia sem√¢ntica
    cache_dir="./cache_contexto"
)
```

## üéØ Pr√≥ximos Passos

1. **Sistema de Contexto Expandido**
   - ‚úÖ Implementado sistema para 2M tokens
   - ‚úÖ Cache inteligente com embeddings
   - ‚úÖ Processamento paralelo otimizado
   - ‚úÖ Instalador autom√°tico criado

2. **Otimiza√ß√£o de Performance**
   - Implementar cache Redis para melhor performance
   - Configurar load balancing para m√∫ltiplos modelos
   - Monitorar m√©tricas de lat√™ncia e throughput

3. **Expans√£o de Funcionalidades**
   - Adicionar suporte a mais modelos do OpenRouter
   - Implementar rate limiting personalizado
   - Criar dashboard de monitoramento

4. **Integra√ß√£o com Aplica√ß√µes**
   - Desenvolver SDK para diferentes linguagens
   - Criar plugins para editores de c√≥digo
   - Implementar webhooks para notifica√ß√µes

5. **Seguran√ßa e Compliance**
   - Implementar autentica√ß√£o JWT
   - Adicionar logs de auditoria
   - Configurar backup autom√°tico do cache

## Recursos Adicionais

- [Documenta√ß√£o LiteLLM](https://docs.litellm.ai/)
- [OpenRouter API](https://openrouter.ai/docs)
- [Guia de Contexto Expandido](./GUIA_AUMENTAR_CONTEXTO_LOCAL.md)
- [Exemplo Pr√°tico](./EXEMPLO_PRATICO_USO.md)
- [Sistema de Contexto 2M Tokens](./sistema_contexto_expandido_2m.py)

---

**Documenta√ß√£o criada em:** 2025  
**Vers√£o:** 2.0 (com Sistema de Contexto Expandido)  
**Autor:** Assistente AI

**Nota**: Este sistema foi testado com Windows PowerShell. Para outros sistemas operacionais, ajuste os comandos conforme necess√°rio.