#!/usr/bin/env python3
"""
MQL5 Docs Ingestion (local RAG)

- Crawls https://www.mql5.com/en/docs (or a provided base URL) politely
- Saves raw HTML under tools/mcp-mql5-rag/raw_html
- Writes cleaned Markdown under tools/mcp-mql5-rag/data mirroring URL paths
- Respects robots.txt, rate-limits, retries
- Incremental: skips unchanged pages (based on ETag/Last-Modified/hash cache)

Usage (from repo root):
  python tools/mcp-mql5-rag/ingest_mql5_docs.py \
    --base-url https://www.mql5.com/en/docs \
    --out-dir tools/mcp-mql5-rag/data \
    --raw-dir tools/mcp-mql5-rag/raw_html \
    --concurrency 4 --max-pages 0

Then index with micro-rag-mcp:
  uvx micro-rag-mcp --data-folder ./tools/mcp-mql5-rag/data

Requirements (install with pip or uv):
  httpx beautifulsoup4 lxml (optional but recommended)

Notes:
- Please review and comply with mql5.com Terms of Service and robots.txt.
- For personal/offline reference only. Do not redistribute.
"""

from __future__ import annotations

import argparse
import asyncio
import hashlib
import json
import os
import random
import re
import sys
import time
from pathlib import Path
from typing import Optional, Set
from urllib.parse import urljoin, urlparse, urldefrag
import urllib.robotparser as robotparser

try:
    import httpx
except ImportError:
    print("Missing dependency: httpx. Install with: pip install httpx", file=sys.stderr)
    sys.exit(1)

try:
    from bs4 import BeautifulSoup
except ImportError:
    print("Missing dependency: beautifulsoup4. Install with: pip install beautifulsoup4", file=sys.stderr)
    sys.exit(1)

# Optional: use lxml parser when available for robustness
PARSER = "lxml"
try:
    import lxml  # noqa: F401
except Exception:
    PARSER = "html.parser"

DEFAULT_UA = (
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
)

ALLOWED_NETLOC = {"www.mql5.com", "mql5.com"}

CLEAN_CLASSES_REGEX = re.compile(
    r"(nav|menu|header|footer|sidebar|breadcrumbs|bread|ads|social|share)",
    re.I,
)

MD_HEADER = """<!--
Generated by ingest_mql5_docs.py
Source: {url}
Fetched: {fetched_at}
--->
"""

CACHE_FILE = "+ingest_cache.jsonl"
MANIFEST_FILE = "+manifest.jsonl"


def sha256_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()


def url_to_relpath(base: str, url: str) -> str:
    # Strip base prefix, keep path, normalize to safe filename ending with .md
    base_parsed = urlparse(base)
    u = urlparse(url)
    path = u.path
    if path.startswith(base_parsed.path):
        rel = path[len(base_parsed.path) :]
    else:
        # fallback: remove leading slash
        rel = path[1:] if path.startswith("/") else path
    if not rel:
        rel = "index"
    if rel.endswith("/"):
        rel += "index"
    # Remove problematic chars
    rel = rel.replace("..", "_").strip("/")
    return rel + ".md"


def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def clean_html_to_markdown(html: str, url: str) -> str:
    soup = BeautifulSoup(html, PARSER)

    # Remove unwanted tags
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    # Remove common layout elements by tag
    for tag_name in ["nav", "header", "footer", "aside"]:
        for tag in soup.find_all(tag_name):
            tag.decompose()

    # Remove common layout by class/id regex
    for tag in soup.find_all(True):
        cls = " ".join(tag.get("class", [])).lower()
        tid = (tag.get("id") or "").lower()
        if CLEAN_CLASSES_REGEX.search(cls) or CLEAN_CLASSES_REGEX.search(tid):
            tag.decompose()

    # Heuristic: try to focus on main content if <main> exists
    main = soup.find("main") or soup.find("article") or soup

    # Convert headings, paragraphs, lists, code blocks to markdown
    lines: list[str] = []

    def text_of(el) -> str:
        return " ".join(el.get_text(separator=" ", strip=True).split())

    def handle_element(el):
        name = getattr(el, "name", None)
        if name is None:
            return
        if re.match(r"h[1-6]", name or ""):
            level = int(name[1])
            lines.append("#" * level + " " + text_of(el))
            lines.append("")
        elif name == "p":
            t = text_of(el)
            if t:
                lines.append(t)
                lines.append("")
        elif name in ("pre",):
            code = el.get_text("\n", strip=False)
            # try detect language from class
            lang = "mql5" if "mql" in " ".join(el.get("class", [])).lower() else ""
            fence = f"```{lang}" if lang else "```"
            lines.append(fence)
            lines.append(code)
            lines.append("```")
            lines.append("")
        elif name == "code":
            # inline code: replace with backticks in parent processing
            # handled implicitly by p/text extraction; skip
            pass
        elif name in ("ul", "ol"):
            for li in el.find_all("li", recursive=False):
                t = text_of(li)
                if t:
                    prefix = "- " if name == "ul" else "1. "
                    lines.append(prefix + t)
            lines.append("")
        elif name == "table":
            # Simple table to markdown: header + rows if possible
            rows = el.find_all("tr")
            if rows:
                header_cells = rows[0].find_all(["th", "td"]) if rows else []
                if header_cells:
                    header = " | ".join(text_of(c) for c in header_cells)
                    sep = " | ".join(["---"] * len(header_cells))
                    lines.append(header)
                    lines.append(sep)
                    for tr in rows[1:]:
                        cells = tr.find_all(["td", "th"]) or []
                        if cells:
                            row = " | ".join(text_of(c) for c in cells)
                            lines.append(row)
                    lines.append("")
        else:
            # Recurse into children for other containers
            for child in el.children:
                if getattr(child, "name", None):
                    handle_element(child)

    handle_element(main)

    # Prepend metadata header
    md = MD_HEADER.format(url=url, fetched_at=time.strftime("%Y-%m-%d %H:%M:%S")) + "\n" + "\n".join(lines)
    # Collapse excessive blank lines
    md = re.sub(r"\n{3,}", "\n\n", md)
    return md.strip() + "\n"


class Cache:
    def __init__(self, path: Path):
        self.path = path
        self._seen: dict[str, dict] = {}
        if path.exists():
            with path.open("r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        rec = json.loads(line)
                        self._seen[rec.get("url")] = rec
                    except Exception:
                        continue

    def get(self, url: str) -> Optional[dict]:
        return self._seen.get(url)

    def update(self, url: str, etag: Optional[str], last_mod: Optional[str], sha: str):
        rec = {"url": url, "etag": etag, "last_modified": last_mod, "sha": sha, "ts": time.time()}
        self._seen[url] = rec
        with self.path.open("a", encoding="utf-8") as f:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")


async def crawl(
    base_url: str,
    out_dir: Path,
    raw_dir: Path,
    concurrency: int,
    max_pages: int,
    delay_min: float,
    delay_max: float,
    user_agent: str,
):
    ensure_dir(out_dir)
    ensure_dir(raw_dir)
    cache = Cache(raw_dir / CACHE_FILE)
    manifest_f = (raw_dir / MANIFEST_FILE).open("a", encoding="utf-8")

    # robots.txt
    rp = robotparser.RobotFileParser()
    robots_url = urljoin(base_url, "/robots.txt")
    try:
        rp.set_url(robots_url)
        rp.read()
    except Exception:
        pass  # continue best-effort

    base_parsed = urlparse(base_url)

    async def allowed(u: str) -> bool:
        up = urlparse(u)
        if up.scheme not in {"http", "https"}:
            return False
        if up.netloc not in ALLOWED_NETLOC:
            return False
        if not up.path.startswith(base_parsed.path):
            return False
        try:
            return rp.can_fetch(user_agent, u)
        except Exception:
            return True

    seen: Set[str] = set()
    q: asyncio.Queue[str] = asyncio.Queue()
    await q.put(base_url)

    sem = asyncio.Semaphore(concurrency)

    async with httpx.AsyncClient(headers={"User-Agent": user_agent}, timeout=20.0, follow_redirects=True) as client:
        async def fetch_process(url: str):
            if url in seen:
                return
            seen.add(url)
            if max_pages and len(seen) > max_pages:
                return
            if not await allowed(url):
                return

            await asyncio.sleep(random.uniform(delay_min, delay_max))

            headers = {}
            c_rec = cache.get(url)
            if c_rec:
                if c_rec.get("etag"):
                    headers["If-None-Match"] = c_rec["etag"]
                if c_rec.get("last_modified"):
                    headers["If-Modified-Since"] = c_rec["last_modified"]

            try:
                resp = await client.get(url, headers=headers)
            except Exception as e:
                print(f"GET failed {url}: {e}")
                return

            if resp.status_code == 304:
                # unchanged, but still parse links from cache? skip to be safe
                return
            if resp.status_code != 200 or not resp.content:
                print(f"Skip {url} status={resp.status_code}")
                return

            content = resp.content
            etag = resp.headers.get("ETag")
            last_mod = resp.headers.get("Last-Modified")
            sha = sha256_bytes(content)

            # Skip if unchanged by hash
            if c_rec and c_rec.get("sha") == sha:
                return

            # Save raw html
            raw_path = raw_dir / (url_to_relpath(base_url, url).replace(".md", ".html"))
            ensure_dir(raw_path.parent)
            raw_path.write_bytes(content)

            # Convert to markdown
            try:
                md = clean_html_to_markdown(content.decode("utf-8", errors="ignore"), url)
            except Exception as e:
                print(f"HTML->MD failed {url}: {e}")
                return

            out_path = out_dir / url_to_relpath(base_url, url)
            ensure_dir(out_path.parent)
            out_path.write_text(md, encoding="utf-8")

            cache.update(url, etag, last_mod, sha)

            # Write manifest record
            manifest_f.write(json.dumps({
                "url": url,
                "raw": str(raw_path),
                "md": str(out_path),
                "sha": sha,
                "ts": time.time(),
                "status": resp.status_code,
            }) + "\n")
            manifest_f.flush()

            # Enqueue new links
            try:
                doc = BeautifulSoup(content, PARSER)
                for a in doc.select("a[href]"):
                    href = a.get("href")
                    if not href:
                        continue
                    href = urljoin(url, href)
                    href, _ = urldefrag(href)  # drop anchors
                    up = urlparse(href)
                    if up.netloc in ALLOWED_NETLOC and up.path.startswith(base_parsed.path):
                        if href not in seen:
                            await q.put(href)
            except Exception:
                pass

        async def worker():
            while True:
                try:
                    url = await asyncio.wait_for(q.get(), timeout=2.0)
                except asyncio.TimeoutError:
                    return
                async with sem:
                    await fetch_process(url)
                q.task_done()

        workers = [asyncio.create_task(worker()) for _ in range(concurrency)]
        await q.join()
        for w in workers:
            w.cancel()

    manifest_f.close()


def main():
    ap = argparse.ArgumentParser(description="Ingest MQL5 documentation into local Markdown for RAG")
    ap.add_argument("--base-url", default="https://www.mql5.com/en/docs", help="Root URL to crawl (default: en/docs)")
    ap.add_argument("--out-dir", default="tools/mcp-mql5-rag/data", help="Directory to write Markdown files")
    ap.add_argument("--raw-dir", default="tools/mcp-mql5-rag/raw_html", help="Directory to store raw HTML")
    ap.add_argument("--concurrency", type=int, default=4)
    ap.add_argument("--max-pages", type=int, default=0, help="0 = unlimited (within base path)")
    ap.add_argument("--delay-min", type=float, default=0.5)
    ap.add_argument("--delay-max", type=float, default=1.5)
    ap.add_argument("--user-agent", default=DEFAULT_UA)
    args = ap.parse_args()

    out_dir = Path(args.out_dir)
    raw_dir = Path(args.raw_dir)

    print(f"Base URL: {args.base_url}")
    print(f"Output dir: {out_dir}")
    print(f"Raw dir: {raw_dir}")
    print(f"Concurrency: {args.concurrency}  Max pages: {args.max_pages}")

    try:
        asyncio.run(
            crawl(
                base_url=args.base_url,
                out_dir=out_dir,
                raw_dir=raw_dir,
                concurrency=args.concurrency,
                max_pages=args.max_pages,
                delay_min=args.delay_min,
                delay_max=args.delay_max,
                user_agent=args.user_agent,
            )
        )
    except KeyboardInterrupt:
        print("Interrupted")
        return 130

    print("Done. You can now run: uvx micro-rag-mcp --data-folder ./tools/mcp-mql5-rag/data")


if __name__ == "__main__":
    sys.exit(main() or 0)
