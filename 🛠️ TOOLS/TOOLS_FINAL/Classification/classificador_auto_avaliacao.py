#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ðŸŽ¯ CLASSIFICADOR COM AUTO-AVALIAÃ‡ÃƒO CONTÃNUA
Sistema inteligente que se auto-monitora e melhora continuamente

Recursos:
- Auto-avaliaÃ§Ã£o a cada N arquivos
- MÃ©tricas de qualidade em tempo real
- Ajustes automÃ¡ticos de parÃ¢metros
- RelatÃ³rios de melhoria contÃ­nua
- DetecÃ§Ã£o de padrÃµes emergentes
"""

import os
import json
import re
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass, asdict
import statistics

@dataclass
class MetricasAutoAvaliacao:
    """MÃ©tricas para auto-avaliaÃ§Ã£o do processo"""
    total_arquivos: int = 0
    deteccoes_corretas: int = 0
    deteccoes_incertas: int = 0
    casos_especiais_detectados: int = 0
    nomenclatura_consistente: int = 0
    ftmo_compliance_detectado: int = 0
    confidence_scores: List[float] = None
    tempo_processamento: List[float] = None
    
    def __post_init__(self):
        if self.confidence_scores is None:
            self.confidence_scores = []
        if self.tempo_processamento is None:
            self.tempo_processamento = []
    
    @property
    def taxa_precisao(self) -> float:
        """Taxa de precisÃ£o das detecÃ§Ãµes"""
        if self.total_arquivos == 0:
            return 0.0
        return (self.deteccoes_corretas / self.total_arquivos) * 100
    
    @property
    def confidence_media(self) -> float:
        """Confidence score mÃ©dio"""
        return statistics.mean(self.confidence_scores) if self.confidence_scores else 0.0
    
    @property
    def tempo_medio(self) -> float:
        """Tempo mÃ©dio de processamento por arquivo"""
        return statistics.mean(self.tempo_processamento) if self.tempo_processamento else 0.0

class AutoAvaliadorQualidade:
    """Sistema de auto-avaliaÃ§Ã£o contÃ­nua"""
    
    def __init__(self, intervalo_avaliacao: int = 10):
        self.intervalo_avaliacao = intervalo_avaliacao
        self.metricas = MetricasAutoAvaliacao()
        self.historico_avaliacoes = []
        self.padroes_emergentes = {}
        self.ajustes_realizados = []
        
        # Thresholds para qualidade
        self.thresholds = {
            'confidence_minimo': 70.0,
            'taxa_precisao_minima': 85.0,
            'tempo_maximo_por_arquivo': 5.0,
            'casos_especiais_minimos': 5.0
        }
    
    def avaliar_deteccao_tipo(self, codigo: str, tipo_detectado: str) -> bool:
        """Avalia se a detecÃ§Ã£o de tipo estÃ¡ correta"""
        # PadrÃµes mais rigorosos para validaÃ§Ã£o
        padroes_ea = [
            r'OnTick\s*\(',
            r'OrderSend\s*\(',
            r'trade\.Buy\s*\(',
            r'trade\.Sell\s*\(',
            r'PositionOpen\s*\(',
            r'#property\s+strict'
        ]
        
        padroes_indicator = [
            r'OnCalculate\s*\(',
            r'SetIndexBuffer\s*\(',
            r'IndicatorBuffers\s*\(',
            r'#property\s+indicator',
            r'PlotIndexSetInteger\s*\('
        ]
        
        padroes_script = [
            r'OnStart\s*\(',
            r'#property\s+script',
            r'void\s+OnStart\s*\('
        ]
        
        # Contagem de matches
        ea_matches = sum(1 for p in padroes_ea if re.search(p, codigo, re.IGNORECASE))
        ind_matches = sum(1 for p in padroes_indicator if re.search(p, codigo, re.IGNORECASE))
        scr_matches = sum(1 for p in padroes_script if re.search(p, codigo, re.IGNORECASE))
        
        # Determina tipo esperado
        if ea_matches >= 2:
            tipo_esperado = 'EA'
        elif ind_matches >= 2:
            tipo_esperado = 'Indicator'
        elif scr_matches >= 1:
            tipo_esperado = 'Script'
        else:
            tipo_esperado = 'Unknown'
        
        return tipo_detectado == tipo_esperado
    
    def avaliar_nomenclatura(self, nome_original: str, nome_sugerido: str) -> bool:
        """Avalia se a nomenclatura segue o padrÃ£o"""
        # PadrÃ£o: [PREFIXO]_[NOME]_v[VER]_[MERCADO].[EXT]
        padrao = r'^(EA|IND|SCR|STR|LIB)_[A-Za-z0-9]+_v\d+\.\d+_[A-Z0-9]+\.(mq[45]|pine)$'
        return bool(re.match(padrao, nome_sugerido))
    
    def avaliar_ftmo_compliance(self, analise_ftmo: Dict) -> bool:
        """Avalia se a anÃ¡lise FTMO estÃ¡ correta"""
        score = analise_ftmo.get('score', 0)
        nivel = analise_ftmo.get('compliance_level', 'Non_Compliant')
        
        # ValidaÃ§Ã£o de consistÃªncia
        if score >= 6 and nivel != 'FTMO_Ready':
            return False
        if score >= 4 and nivel == 'Non_Compliant':
            return False
        if score < 2 and nivel == 'FTMO_Ready':
            return False
        
        return True
    
    def detectar_padroes_emergentes(self, metadata: Dict):
        """Detecta novos padrÃµes que podem indicar novas categorias"""
        estrategia = metadata.get('classification', {}).get('strategy', 'Unknown')
        mercado = metadata.get('classification', {}).get('markets', ['Unknown'])[0]
        
        # Registra combinaÃ§Ãµes
        combinacao = f"{estrategia}_{mercado}"
        if combinacao not in self.padroes_emergentes:
            self.padroes_emergentes[combinacao] = 0
        self.padroes_emergentes[combinacao] += 1
    
    def registrar_processamento(self, metadata: Dict, tempo_processamento: float):
        """Registra mÃ©tricas de um arquivo processado"""
        import time
        start_time = time.time()
        
        self.metricas.total_arquivos += 1
        self.metricas.tempo_processamento.append(tempo_processamento)
        
        # Avalia detecÃ§Ã£o de tipo
        codigo = metadata.get('file_info', {}).get('content_preview', '')
        tipo_detectado = metadata.get('classification', {}).get('file_type', 'Unknown')
        
        if self.avaliar_deteccao_tipo(codigo, tipo_detectado):
            self.metricas.deteccoes_corretas += 1
        else:
            self.metricas.deteccoes_incertas += 1
        
        # Avalia nomenclatura
        nome_original = metadata.get('file_info', {}).get('original_name', '')
        nome_sugerido = metadata.get('organization', {}).get('suggested_name', '')
        
        if self.avaliar_nomenclatura(nome_original, nome_sugerido):
            self.metricas.nomenclatura_consistente += 1
        
        # Avalia FTMO compliance
        analise_ftmo = metadata.get('ftmo_analysis', {})
        if self.avaliar_ftmo_compliance(analise_ftmo):
            self.metricas.ftmo_compliance_detectado += 1
        
        # Registra confidence score
        confidence = metadata.get('analysis_metadata', {}).get('confidence_score', 0)
        self.metricas.confidence_scores.append(confidence)
        
        # Detecta casos especiais
        if metadata.get('special_analysis', {}).get('is_exceptional', False):
            self.metricas.casos_especiais_detectados += 1
        
        # Detecta padrÃµes emergentes
        self.detectar_padroes_emergentes(metadata)
        
        # Auto-avaliaÃ§Ã£o periÃ³dica
        if self.metricas.total_arquivos % self.intervalo_avaliacao == 0:
            self.executar_auto_avaliacao()
    
    def executar_auto_avaliacao(self) -> Dict:
        """Executa auto-avaliaÃ§Ã£o completa"""
        print(f"\nðŸ” AUTO-AVALIAÃ‡ÃƒO - Arquivo {self.metricas.total_arquivos}")
        print("=" * 50)
        
        avaliacao = {
            'timestamp': datetime.now().isoformat(),
            'arquivos_processados': self.metricas.total_arquivos,
            'metricas': {
                'taxa_precisao': self.metricas.taxa_precisao,
                'confidence_media': self.metricas.confidence_media,
                'tempo_medio': self.metricas.tempo_medio,
                'casos_especiais': self.metricas.casos_especiais_detectados
            },
            'status_qualidade': self.avaliar_qualidade_geral(),
            'ajustes_sugeridos': self.sugerir_ajustes(),
            'padroes_emergentes': self.identificar_padroes_emergentes()
        }
        
        self.historico_avaliacoes.append(avaliacao)
        self.imprimir_relatorio_avaliacao(avaliacao)
        
        return avaliacao
    
    def avaliar_qualidade_geral(self) -> str:
        """Avalia qualidade geral do processo"""
        score = 0
        
        # Taxa de precisÃ£o
        if self.metricas.taxa_precisao >= self.thresholds['taxa_precisao_minima']:
            score += 25
        elif self.metricas.taxa_precisao >= 70:
            score += 15
        
        # Confidence mÃ©dio
        if self.metricas.confidence_media >= self.thresholds['confidence_minimo']:
            score += 25
        elif self.metricas.confidence_media >= 50:
            score += 15
        
        # Tempo de processamento
        if self.metricas.tempo_medio <= self.thresholds['tempo_maximo_por_arquivo']:
            score += 25
        elif self.metricas.tempo_medio <= 10:
            score += 15
        
        # DetecÃ§Ã£o de casos especiais
        taxa_especiais = (self.metricas.casos_especiais_detectados / self.metricas.total_arquivos) * 100
        if taxa_especiais >= self.thresholds['casos_especiais_minimos']:
            score += 25
        elif taxa_especiais >= 2:
            score += 15
        
        if score >= 90:
            return "EXCELENTE"
        elif score >= 70:
            return "BOM"
        elif score >= 50:
            return "REGULAR"
        else:
            return "PRECISA_MELHORAR"
    
    def sugerir_ajustes(self) -> List[str]:
        """Sugere ajustes baseados nas mÃ©tricas"""
        ajustes = []
        
        if self.metricas.taxa_precisao < self.thresholds['taxa_precisao_minima']:
            ajustes.append("ðŸ”§ Melhorar padrÃµes de detecÃ§Ã£o de tipo")
            ajustes.append("ðŸ“š Adicionar mais keywords especÃ­ficas")
        
        if self.metricas.confidence_media < self.thresholds['confidence_minimo']:
            ajustes.append("âš¡ Refinar algoritmo de confidence scoring")
            ajustes.append("ðŸŽ¯ Adicionar validaÃ§Ã£o cruzada de padrÃµes")
        
        if self.metricas.tempo_medio > self.thresholds['tempo_maximo_por_arquivo']:
            ajustes.append("ðŸš€ Otimizar performance de anÃ¡lise")
            ajustes.append("ðŸ’¾ Implementar cache de padrÃµes")
        
        taxa_especiais = (self.metricas.casos_especiais_detectados / self.metricas.total_arquivos) * 100
        if taxa_especiais < self.thresholds['casos_especiais_minimos']:
            ajustes.append("ðŸ” Melhorar detecÃ§Ã£o de casos especiais")
            ajustes.append("â­ Adicionar mais padrÃµes de qualidade")
        
        return ajustes
    
    def identificar_padroes_emergentes(self) -> List[str]:
        """Identifica padrÃµes emergentes que podem virar novas categorias"""
        emergentes = []
        
        for padrao, count in self.padroes_emergentes.items():
            if count >= 5:  # Threshold para nova categoria
                emergentes.append(f"ðŸ“ˆ PadrÃ£o '{padrao}': {count} ocorrÃªncias - Considerar nova categoria")
        
        return emergentes
    
    def imprimir_relatorio_avaliacao(self, avaliacao: Dict):
        """Imprime relatÃ³rio de auto-avaliaÃ§Ã£o"""
        print(f"ðŸ“Š Taxa de PrecisÃ£o: {avaliacao['metricas']['taxa_precisao']:.1f}%")
        print(f"ðŸŽ¯ Confidence MÃ©dio: {avaliacao['metricas']['confidence_media']:.1f}%")
        print(f"â±ï¸ Tempo MÃ©dio: {avaliacao['metricas']['tempo_medio']:.2f}s/arquivo")
        print(f"â­ Casos Especiais: {avaliacao['metricas']['casos_especiais']}")
        print(f"ðŸ† Status Geral: {avaliacao['status_qualidade']}")
        
        if avaliacao['ajustes_sugeridos']:
            print("\nðŸ”§ AJUSTES SUGERIDOS:")
            for ajuste in avaliacao['ajustes_sugeridos']:
                print(f"   {ajuste}")
        
        if avaliacao['padroes_emergentes']:
            print("\nðŸ“ˆ PADRÃ•ES EMERGENTES:")
            for padrao in avaliacao['padroes_emergentes']:
                print(f"   {padrao}")
        
        print("=" * 50)
    
    def gerar_relatorio_final(self) -> Dict:
        """Gera relatÃ³rio final de auto-avaliaÃ§Ã£o"""
        return {
            'resumo_geral': {
                'total_arquivos': self.metricas.total_arquivos,
                'taxa_precisao_final': self.metricas.taxa_precisao,
                'confidence_media_final': self.metricas.confidence_media,
                'tempo_total': sum(self.metricas.tempo_processamento),
                'casos_especiais_total': self.metricas.casos_especiais_detectados
            },
            'evolucao_qualidade': self.historico_avaliacoes,
            'padroes_emergentes_finais': self.padroes_emergentes,
            'ajustes_realizados': self.ajustes_realizados,
            'recomendacoes_futuras': self.gerar_recomendacoes_futuras()
        }
    
    def gerar_recomendacoes_futuras(self) -> List[str]:
        """Gera recomendaÃ§Ãµes para melhorias futuras"""
        recomendacoes = []
        
        # AnÃ¡lise de tendÃªncias
        if len(self.historico_avaliacoes) >= 3:
            ultimas_precisoes = [a['metricas']['taxa_precisao'] for a in self.historico_avaliacoes[-3:]]
            if ultimas_precisoes[-1] < ultimas_precisoes[0]:
                recomendacoes.append("ðŸ“‰ PrecisÃ£o em declÃ­nio - Revisar padrÃµes de detecÃ§Ã£o")
        
        # PadrÃµes emergentes
        for padrao, count in self.padroes_emergentes.items():
            if count >= 10:
                recomendacoes.append(f"ðŸ†• Criar categoria especÃ­fica para '{padrao}' ({count} arquivos)")
        
        # Performance
        if self.metricas.tempo_medio > 3:
            recomendacoes.append("âš¡ Implementar otimizaÃ§Ãµes de performance")
        
        return recomendacoes

# Exemplo de uso integrado
if __name__ == "__main__":
    # DemonstraÃ§Ã£o do sistema de auto-avaliaÃ§Ã£o
    avaliador = AutoAvaliadorQualidade(intervalo_avaliacao=5)
    
    print("ðŸŽ¯ SISTEMA DE AUTO-AVALIAÃ‡ÃƒO ATIVADO")
    print("Monitoramento contÃ­nuo da qualidade do processo...\n")
    
    # SimulaÃ§Ã£o de processamento com auto-avaliaÃ§Ã£o
    arquivos_teste = [
        "EA_Test1.mq4", "IND_Test2.mq4", "SCR_Test3.mq4",
        "EA_Test4.mq5", "IND_Test5.mq5", "Unknown_Test6.mq4"
    ]
    
    for i, arquivo in enumerate(arquivos_teste):
        # Simula metadata de processamento
        metadata_simulado = {
            'file_info': {
                'original_name': arquivo,
                'content_preview': 'OnTick() { OrderSend(); }' if 'EA_' in arquivo else 'OnCalculate() { SetIndexBuffer(); }'
            },
            'classification': {
                'file_type': 'EA' if 'EA_' in arquivo else 'Indicator' if 'IND_' in arquivo else 'Script',
                'strategy': 'scalping',
                'markets': ['EURUSD']
            },
            'organization': {
                'suggested_name': arquivo.replace('.mq', '_v1.0_EURUSD.mq')
            },
            'ftmo_analysis': {
                'score': 5,
                'compliance_level': 'Partially_Compliant'
            },
            'analysis_metadata': {
                'confidence_score': 85.0
            },
            'special_analysis': {
                'is_exceptional': i % 3 == 0  # Simula casos especiais
            }
        }
        
        # Registra processamento
        avaliador.registrar_processamento(metadata_simulado, 2.5)
        
        print(f"âœ… Processado: {arquivo}")
    
    # RelatÃ³rio final
    print("\nðŸ“‹ RELATÃ“RIO FINAL DE AUTO-AVALIAÃ‡ÃƒO")
    relatorio_final = avaliador.gerar_relatorio_final()
    print(json.dumps(relatorio_final, indent=2, ensure_ascii=False))