#!/usr/bin/env python3
"""
üîç EA Optimizer AI - Validator Module
Valida√ß√£o e backtesting autom√°tico dos resultados de otimiza√ß√£o
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
import logging
from pathlib import Path
import json
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EAValidator:
    """Validador de resultados de otimiza√ß√£o de EA"""

    def __init__(self, symbol: str = "XAUUSD", timeframe: str = "M5"):
        """
        Inicializa o validador

        Args:
            symbol: S√≠mbolo de trading
            timeframe: Timeframe para an√°lise
        """
        self.symbol = symbol
        self.timeframe = timeframe
        self.validation_results = []

    def validate_optimization_results(self,
                                    optimization_results: Dict[str, Any],
                                    validation_method: str = "walk_forward") -> Dict[str, Any]:
        """
        Valida resultados da otimiza√ß√£o usando m√∫ltiplos m√©todos

        Args:
            optimization_results: Resultados da otimiza√ß√£o
            validation_method: M√©todo de valida√ß√£o (walk_forward, cross_val, monte_carlo)

        Returns:
            Resultados da valida√ß√£o
        """
        logger.info(f"üîç Validando otimiza√ß√£o usando m√©todo: {validation_method}")

        best_params = optimization_results.get('best_params', {})
        best_score = optimization_results.get('best_score', 0)

        if validation_method == "walk_forward":
            validation_results = self._walk_forward_validation(best_params)
        elif validation_method == "cross_validation":
            validation_results = self._cross_validation(optimization_results)
        elif validation_method == "monte_carlo":
            validation_results = self._monte_carlo_validation(best_params)
        else:
            raise ValueError(f"M√©todo de valida√ß√£o desconhecido: {validation_method}")

        # Adicionar m√©tricas de valida√ß√£o
        validation_results.update({
            'original_score': best_score,
            'validation_method': validation_method,
            'symbol': self.symbol,
            'timeframe': self.timeframe,
            'validation_timestamp': datetime.now().isoformat()
        })

        logger.info(f"‚úÖ Valida√ß√£o conclu√≠da: Score validado = {validation_results.get('validated_score', 0):.4f}")
        return validation_results

    def _walk_forward_validation(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida√ß√£o Walk-Forward

        Args:
            params: Par√¢metros do EA

        Returns:
            Resultados da valida√ß√£o walk-forward
        """
        logger.info("üö∂ Executando valida√ß√£o Walk-Forward...")

        # Simular dados hist√≥ricos (em implementa√ß√£o real, usar dados reais)
        n_periods = 252  # Um ano de dias de trading
        train_size = 180  # 6 meses para treinamento
        test_size = 30    # 1 m√™s para teste

        walk_forward_results = []

        for period in range(0, n_periods - train_size - test_size, test_size):
            # Dados de treinamento (per√≠odo anterior)
            train_start = period
            train_end = period + train_size

            # Dados de teste (per√≠odo seguinte)
            test_start = train_end
            test_end = test_start + test_size

            # Simular performance no per√≠odo de teste
            test_performance = self._simulate_period_performance(
                params, test_start, test_end, "test"
            )

            walk_forward_results.append({
                'period_start': test_start,
                'period_end': test_end,
                'train_start': train_start,
                'train_end': train_end,
                'test_score': test_performance['score'],
                'test_profit': test_performance['profit'],
                'test_drawdown': test_performance['drawdown'],
                'test_winrate': test_performance['winrate'],
                'test_trades': test_performance['trades']
            })

        # Calcular estat√≠sticas agregadas
        avg_score = np.mean([r['test_score'] for r in walk_forward_results])
        std_score = np.std([r['test_score'] for r in walk_forward_results])
        avg_profit = np.mean([r['test_profit'] for r in walk_forward_results])
        avg_drawdown = np.mean([r['test_drawdown'] for r in walk_forward_results])
        consistency = self._calculate_consistency(walk_forward_results)

        return {
            'validation_type': 'walk_forward',
            'validated_score': avg_score,
            'score_std': std_score,
            'consistency_score': consistency,
            'avg_profit': avg_profit,
            'avg_drawdown': avg_drawdown,
            'period_results': walk_forward_results,
            'total_periods': len(walk_forward_results),
            'robustness_score': self._calculate_robustness_score(walk_forward_results)
        }

    def _cross_validation(self, optimization_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida√ß√£o Cruzada K-Fold

        Args:
            optimization_results: Resultados completos da otimiza√ß√£o

        Returns:
            Resultados da valida√ß√£o cruzada
        """
        logger.info("üîÑ Executando valida√ß√£o Cruzada...")

        trials = optimization_results.get('optimization_history', [])
        if len(trials) < 20:
            logger.warning("‚ö†Ô∏è N√∫mero insuficiente de trials para valida√ß√£o cruzada")
            return {'validation_type': 'cross_validation', 'error': 'Insufficient data'}

        # Extrair scores dos trials
        scores = [trial['score'] for trial in trials]

        # Valida√ß√£o cruzada 5-fold
        k_folds = 5
        fold_size = len(scores) // k_folds
        cv_scores = []

        for fold in range(k_folds):
            start_idx = fold * fold_size
            end_idx = (fold + 1) * fold_size if fold < k_folds - 1 else len(scores)

            # Valida√ß√£o no fold
            fold_scores = scores[start_idx:end_idx]
            cv_scores.append({
                'fold': fold + 1,
                'mean_score': np.mean(fold_scores),
                'std_score': np.std(fold_scores),
                'min_score': np.min(fold_scores),
                'max_score': np.max(fold_scores),
                'samples': len(fold_scores)
            })

        # Estat√≠sticas da valida√ß√£o cruzada
        overall_mean = np.mean([f['mean_score'] for f in cv_scores])
        overall_std = np.std([f['mean_score'] for f in cv_scores])

        return {
            'validation_type': 'cross_validation',
            'validated_score': overall_mean,
            'score_std': overall_std,
            'cv_folds': k_folds,
            'fold_results': cv_scores,
            'stability_score': 1.0 - (overall_std / overall_mean) if overall_mean > 0 else 0,
            'confidence_interval': [
                overall_mean - 1.96 * overall_std / np.sqrt(k_folds),
                overall_mean + 1.96 * overall_std / np.sqrt(k_folds)
            ]
        }

    def _monte_carlo_validation(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida√ß√£o Monte Carlo

        Args:
            params: Par√¢metros do EA

        Returns:
            Resultados da valida√ß√£o Monte Carlo
        """
        logger.info("üé≤ Executando valida√ß√£o Monte Carlo...")

        n_simulations = 1000
        simulation_results = []

        for sim in range(n_simulations):
            # Adicionar ru√≠do aos par√¢metros
            noisy_params = self._add_parameter_noise(params, noise_level=0.1)

            # Simular performance
            performance = self._simulate_period_performance(
                noisy_params, 0, 30, f"simulation_{sim}"
            )

            simulation_results.append(performance['score'])

        # Estat√≠sticas das simula√ß√µes
        mean_score = np.mean(simulation_results)
        std_score = np.std(simulation_results)
        percentile_5 = np.percentile(simulation_results, 5)
        percentile_95 = np.percentile(simulation_results, 95)

        # Calcular probabilidade de sucesso
        success_threshold = params.get('min_acceptable_score', 50)
        success_probability = np.mean([s >= success_threshold for s in simulation_results])

        return {
            'validation_type': 'monte_carlo',
            'validated_score': mean_score,
            'score_std': std_score,
            'percentile_5': percentile_5,
            'percentile_95': percentile_95,
            'success_probability': success_probability,
            'n_simulations': n_simulations,
            'stability_score': 1.0 - (std_score / mean_score) if mean_score > 0 else 0,
            'simulation_results': simulation_results[:100]  # Salvar apenas as 100 primeiras para economizar espa√ßo
        }

    def _simulate_period_performance(self,
                                   params: Dict[str, Any],
                                   start_period: int,
                                   end_period: int,
                                   period_name: str) -> Dict[str, float]:
        """
        Simula performance do EA em um per√≠odo espec√≠fico

        Args:
            params: Par√¢metros do EA
            start_period: Per√≠odo inicial
            end_period: Per√≠odo final
            period_name: Nome do per√≠odo

        Returns:
            M√©tricas de performance simuladas
        """
        # Dura√ß√£o do per√≠odo
        period_length = end_period - start_period

        # Par√¢metros de risco
        risk_reward = params.get('take_profit', 200) / max(params.get('stop_loss', 100), 1)
        risk_factor = params.get('risk_factor', 1.5)
        lot_size = params.get('lot_size', 0.01)

        # Simular base de performance
        base_score = 50.0  # Score base

        # Ajustar score baseado nos par√¢metros
        score_adjustments = 0

        # Risk/Reward Ratio
        if risk_reward > 2.0:
            score_adjustments += 20
        elif risk_reward > 1.5:
            score_adjustments += 10
        elif risk_reward < 1.0:
            score_adjustments -= 20

        # Risk Factor (valores moderados s√£o melhores)
        if 1.0 <= risk_factor <= 2.0:
            score_adjustments += 15
        elif risk_factor > 2.5:
            score_adjustments -= 10

        # ATR Multiplier
        atr_multiplier = params.get('atr_multiplier', 1.5)
        if 1.2 <= atr_multiplier <= 2.0:
            score_adjustments += 10

        # Adicionar variabilidade aleat√≥ria (simular condi√ß√£o de mercado)
        market_condition = np.random.normal(0, 15)
        noise = np.random.normal(0, 5)

        # Calcular m√©tricas
        final_score = base_score + score_adjustments + market_condition + noise
        final_score = max(0, final_score)  # Score n√£o pode ser negativo

        # Simular outras m√©tricas baseadas no score
        profit = final_score * lot_size * period_length * 0.1
        drawdown = max(5, 30 - final_score * 0.2) + np.random.normal(0, 3)
        winrate = min(80, max(20, 40 + final_score * 0.3 + np.random.normal(0, 5)))
        trades = int(period_length * np.random.uniform(0.5, 2.0))

        return {
            'score': final_score,
            'profit': profit,
            'drawdown': max(0, drawdown),
            'winrate': winrate,
            'trades': trades,
            'period_name': period_name
        }

    def _add_parameter_noise(self, params: Dict[str, Any], noise_level: float) -> Dict[str, Any]:
        """
        Adiciona ru√≠do aos par√¢metros para simula√ß√£o Monte Carlo

        Args:
            params: Par√¢metros originais
            noise_level: N√≠vel de ru√≠do (0-1)

        Returns:
            Par√¢metros com ru√≠do adicionado
        """
        noisy_params = params.copy()

        for key, value in params.items():
            if isinstance(value, (int, float)):
                # Adicionar ru√≠do gaussiano
                noise = np.random.normal(0, value * noise_level)
                noisy_value = value + noise

                # Manter dentro de limites razo√°veis
                if 'stop_loss' in key or 'take_profit' in key:
                    noisy_value = max(10, noisy_value)
                elif 'risk_factor' in key or 'atr_multiplier' in key:
                    noisy_value = max(0.1, min(5.0, noisy_value))
                elif 'period' in key:
                    noisy_value = max(5, min(100, noisy_value))
                elif 'lot_size' in key:
                    noisy_value = max(0.01, min(1.0, noisy_value))

                noisy_params[key] = noisy_value

        return noisy_params

    def _calculate_consistency(self, results: List[Dict[str, Any]]) -> float:
        """
        Calcula consist√™ncia dos resultados across per√≠odos

        Args:
            results: Lista de resultados por per√≠odo

        Returns:
            Score de consist√™ncia (0-1)
        """
        if len(results) < 2:
            return 1.0

        scores = [r['test_score'] for r in results]
        mean_score = np.mean(scores)
        std_score = np.std(scores)

        # Consist√™ncia = 1 - (coeficiente de varia√ß√£o)
        consistency = 1.0 - (std_score / mean_score) if mean_score > 0 else 0
        return max(0, min(1.0, consistency))

    def _calculate_robustness_score(self, results: List[Dict[str, Any]]) -> float:
        """
        Calcula score de robustez da estrat√©gia

        Args:
            results: Lista de resultados por per√≠odo

        Returns:
            Score de robustez (0-100)
        """
        if not results:
            return 0

        scores = [r['test_score'] for r in results]
        profits = [r['test_profit'] for r in results]
        drawdowns = [r['test_drawdown'] for r in results]

        # Fatores de robustez
        avg_score = np.mean(scores)
        positive_periods = sum(1 for p in profits if p > 0) / len(profits)
        avg_drawdown = np.mean(drawdowns)
        score_consistency = self._calculate_consistency(results)

        # Score de robustez composto
        robustness = (
            avg_score * 0.3 +                    # Performance m√©dia
            positive_periods * 20 * 0.3 +        # Consist√™ncia de lucro
            max(0, 30 - avg_drawdown) * 0.2 +    # Controle de drawdown
            score_consistency * 20 * 0.2         # Consist√™ncia de score
        )

        return max(0, min(100, robustness))

    def generate_validation_report(self,
                                 validation_results: Dict[str, Any],
                                 output_path: str) -> str:
        """
        Gera relat√≥rio detalhado da valida√ß√£o

        Args:
            validation_results: Resultados da valida√ß√£o
            output_path: Caminho para salvar o relat√≥rio

        Returns:
            Caminho do relat√≥rio gerado
        """
        logger.info("üìÑ Gerando relat√≥rio de valida√ß√£o...")

        report_content = self._create_validation_report(validation_results)

        # Salvar relat√≥rio
        report_file = Path(output_path)
        report_file.parent.mkdir(parents=True, exist_ok=True)

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        logger.info(f"üìÑ Relat√≥rio de valida√ß√£o salvo: {report_file}")
        return str(report_file)

    def _create_validation_report(self, validation_results: Dict[str, Any]) -> str:
        """Cria conte√∫do do relat√≥rio de valida√ß√£o"""

        validation_type = validation_results.get('validation_type', 'unknown')
        validated_score = validation_results.get('validated_score', 0)
        original_score = validation_results.get('original_score', 0)

        report = f"""# üîç EA Optimizer AI - Relat√≥rio de Valida√ß√£o

## üìä Sum√°rio da Valida√ß√£o

- **Tipo de Valida√ß√£o**: {validation_type.replace('_', ' ').title()}
- **Data**: {validation_results.get('validation_timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))}
- **S√≠mbolo**: {validation_results.get('symbol', 'N/A')}
- **Timeframe**: {validation_results.get('timeframe', 'N/A')}

## üéØ Resultados Principais

| M√©trica | Valor | Avalia√ß√£o |
|----------|-------|-----------|
| Score Original | {original_score:.4f | } | {'‚úÖ Bom' if original_score > 70 else '‚ö†Ô∏è M√©dio' if original_score > 50 else '‚ùå Baixo'} |
| Score Validado | {validated_score:.4f | } | {'‚úÖ Bom' if validated_score > 70 else '‚ö†Ô∏è M√©dio' if validated_score > 50 else '‚ùå Baixo'} |
| Diferen√ßa | {validated_score - original_score:+.4f | } | {'‚úÖ Est√°vel' if abs(validated_score - original_score) < 10 else '‚ö†Ô∏è Vol√°til'} |
"""

        # Adicionar se√ß√µes espec√≠ficas por tipo de valida√ß√£o
        if validation_type == 'walk_forward':
            report += self._create_walk_forward_section(validation_results)
        elif validation_type == 'cross_validation':
            report += self._create_cross_validation_section(validation_results)
        elif validation_type == 'monte_carlo':
            report += self._create_monte_carlo_section(validation_results)

        # Adicionar avalia√ß√£o final
        report += self._create_final_assessment(validation_results)

        report += f"""
## üìã Recomenda√ß√µes

{self._generate_recommendations(validation_results)}

---

*Relat√≥rio gerado automaticamente pelo EA Optimizer AI Validator*
"""

        return report

    def _create_walk_forward_section(self, results: Dict[str, Any]) -> str:
        """Cria se√ß√£o espec√≠fica para valida√ß√£o walk-forward"""
        avg_profit = results.get('avg_profit', 0)
        avg_drawdown = results.get('avg_drawdown', 0)
        robustness = results.get('robustness_score', 0)
        consistency = results.get('consistency_score', 0)
        total_periods = results.get('total_periods', 0)

        section = f"""
## üö∂ An√°lise Walk-Forward

### M√©tricas de Performance
- **Score M√©dio**: {results.get('validated_score', 0):.4f}
- **Desvio Padr√£o**: {results.get('score_std', 0):.4f}
- **Lucro M√©dio**: ${avg_profit:.2f}
- **Drawdown M√©dio**: {avg_drawdown:.2f}%
- **Total de Per√≠odos**: {total_periods}

### M√©tricas de Robustez
- **Score de Robustez**: {robustness:.2f}/100 {'‚úÖ' if robustness > 70 else '‚ö†Ô∏è' if robustness > 50 else '‚ùå'}
- **Consist√™ncia**: {consistency:.2f}/1.0 {'‚úÖ' if consistency > 0.7 else '‚ö†Ô∏è' if consistency > 0.5 else '‚ùå'}

### An√°lise por Per√≠odo
"""

        period_results = results.get('period_results', [])
        for i, period in enumerate(period_results[:5]):  # Mostrar apenas os 5 primeiros
            section += f"""
- **Per√≠odo {i+1}**: Score={period['test_score']:.2f}, Lucro=${period['test_profit']:.2f}, DD={period['test_drawdown']:.2f}%
"""

        if len(period_results) > 5:
            section += f"- ... e mais {len(period_results) - 5} per√≠odos\n"

        return section

    def _create_cross_validation_section(self, results: Dict[str, Any]) -> str:
        """Cria se√ß√£o espec√≠fica para valida√ß√£o cruzada"""
        cv_folds = results.get('cv_folds', 0)
        stability = results.get('stability_score', 0)
        confidence_interval = results.get('confidence_interval', [0, 0])

        section = f"""
## üîÑ An√°lise de Valida√ß√£o Cruzada

### M√©tricas Principais
- **N√∫mero de Folds**: {cv_folds}
- **Score M√©dio**: {results.get('validated_score', 0):.4f}
- **Desvio Padr√£o**: {results.get('score_std', 0):.4f}
- **Score de Estabilidade**: {stability:.2f}/1.0 {'‚úÖ' if stability > 0.7 else '‚ö†Ô∏è' if stability > 0.5 else '‚ùå'}

### Intervalo de Confian√ßa (95%)
- **Limite Inferior**: {confidence_interval[0]:.4f}
- **Limite Superior**: {confidence_interval[1]:.4f}

### Resultados por Fold
"""

        fold_results = results.get('fold_results', [])
        for fold in fold_results:
            section += f"""
- **Fold {fold['fold']}**: M√©dia={fold['mean_score']:.2f}, Std={fold['std_score']:.2f}, Min={fold['min_score']:.2f}, Max={fold['max_score']:.2f}
"""

        return section

    def _create_monte_carlo_section(self, results: Dict[str, Any]) -> str:
        """Cria se√ß√£o espec√≠fica para valida√ß√£o Monte Carlo"""
        n_simulations = results.get('n_simulations', 0)
        percentile_5 = results.get('percentile_5', 0)
        percentile_95 = results.get('percentile_95', 0)
        success_prob = results.get('success_probability', 0)
        stability = results.get('stability_score', 0)

        section = f"""
## üé≤ An√°lise Monte Carlo

### Estat√≠sticas das Simula√ß√µes
- **N√∫mero de Simula√ß√µes**: {n_simulations}
- **Score M√©dio**: {results.get('validated_score', 0):.4f}
- **Desvio Padr√£o**: {results.get('score_std', 0):.4f}
- **Percentil 5%**: {percentile_5:.4f}
- **Percentil 95%**: {percentile_95:.4f}

### M√©tricas de Risco
- **Probabilidade de Sucesso**: {success_prob:.1%} {'‚úÖ' if success_prob > 0.7 else '‚ö†Ô∏è' if success_prob > 0.5 else '‚ùå'}
- **Score de Estabilidade**: {stability:.2f}/1.0 {'‚úÖ' if stability > 0.7 else '‚ö†Ô∏è' if stability > 0.5 else '‚ùå'}

### An√°lise de Cen√°rios
"""

        if percentile_5 > 50:
            section += "- ‚úÖ **Cen√°rio Otimista**: Even no pior cen√°rio (5%), performance √© aceit√°vel\n"
        elif percentile_5 > 30:
            section += "- ‚ö†Ô∏è **Cen√°rio Moderado**: Pior cen√°rio (5%) ainda pode ser aceit√°vel\n"
        else:
            section += "- ‚ùå **Cen√°rio Pessimista**: Pior cen√°rio (5%) apresenta baixa performance\n"

        if success_prob > 0.8:
            section += "- ‚úÖ **Alta Confian√ßa**: Estrat√©gia tem alta probabilidade de sucesso\n"
        elif success_prob > 0.6:
            section += "- ‚ö†Ô∏è **Confian√ßa Moderada**: Estrat√©gia tem probabilidade moderada de sucesso\n"
        else:
            section += "- ‚ùå **Baixa Confian√ßa**: Estrat√©gia tem baixa probabilidade de sucesso\n"

        return section

    def _create_final_assessment(self, validation_results: Dict[str, Any]) -> str:
        """Cria avalia√ß√£o final dos resultados"""
        validated_score = validation_results.get('validated_score', 0)
        original_score = validation_results.get('original_score', 0)
        validation_type = validation_results.get('validation_type', '')

        # Determinar avalia√ß√£o geral
        if validated_score > 70 and abs(validated_score - original_score) < 15:
            assessment = "‚úÖ **Excelente**"
            assessment_detail = "Estrat√©gia robusta e validada com alta confian√ßa"
        elif validated_score > 50 and abs(validated_score - original_score) < 25:
            assessment = "‚ö†Ô∏è **Aceit√°vel**"
            assessment_detail = "Estrat√©gia razo√°vel, mas com algumas limita√ß√µes"
        else:
            assessment = "‚ùå **Precisa de Melhorias**"
            assessment_detail = "Estrat√©gia apresenta problemas de robustez ou performance"

        return f"""
## üèÅ Avalia√ß√£o Final

{assessment}

{assessment_detail}

### Status de Valida√ß√£o
- **Valida√ß√£o**: {'Aprovada' if validated_score > 50 else 'Reprovada'}
- **Recomenda√ß√£o**: {'Implementar em conta demo' if validated_score > 60 else 'Revisar par√¢metros'}
- **N√≠vel de Confian√ßa**: {'Alto' if abs(validated_score - original_score) < 10 else 'M√©dio' if abs(validated_score - original_score) < 20 else 'Baixo'}
"""

    def _generate_recommendations(self, validation_results: Dict[str, Any]) -> str:
        """Gera recomenda√ß√µes baseadas nos resultados"""
        validated_score = validation_results.get('validated_score', 0)
        original_score = validation_results.get('original_score', 0)
        validation_type = validation_results.get('validation_type', '')

        recommendations = []

        if validated_score < 40:
            recommendations.append("‚ùå **Alta Prioridade**: Revisar completamente a estrat√©gia. Score muito baixo.")
        elif validated_score < 60:
            recommendations.append("‚ö†Ô∏è **M√©dia Prioridade**: Considerar reotimiza√ß√£o com par√¢metros diferentes.")
        else:
            recommendations.append("‚úÖ **Baixa Prioridade**: Estrat√©gia aceit√°vel para testes em conta demo.")

        if abs(validated_score - original_score) > 20:
            recommendations.append("‚ö†Ô∏è **Aten√ß√£o**: Grande diferen√ßa entre score otimizado e validado. Poss√≠vel overfitting.")

        if validation_type == 'walk_forward':
            consistency = validation_results.get('consistency_score', 0)
            if consistency < 0.5:
                recommendations.append("‚ö†Ô∏è **Consist√™ncia**: Baixa consist√™ncia entre per√≠odos. Revisar estabilidade da estrat√©gia.")

        if validation_type == 'monte_carlo':
            success_prob = validation_results.get('success_probability', 0)
            if success_prob < 0.6:
                recommendations.append("‚ö†Ô∏è **Risco**: Baixa probabilidade de sucesso em simula√ß√µes. Considerar estrat√©gia mais conservadora.")

        # Recomenda√ß√µes gerais
        recommendations.extend([
            "üìä **Pr√≥ximo Passo**: Executar backtest em conta demo por pelo menos 1 m√™s.",
            "üîç **Monitoramento**: Acompanhar performance em diferentes condi√ß√µes de mercado.",
            "üìà **Ajustes**: Reotimizar par√¢metros a cada 3-6 meses se necess√°rio."
        ])

        return "\n".join(recommendations)

if __name__ == "__main__":
    # Teste do validador
    validator = EAValidator()

    # Resultados de exemplo
    sample_optimization_results = {
        'best_score': 75.5,
        'best_params': {
            'stop_loss': 120,
            'take_profit': 240,
            'risk_factor': 1.8,
            'atr_multiplier': 1.6
        },
        'optimization_history': [
            {'score': 65.2 + np.random.normal(0, 5)}
            for _ in range(50)
        ]
    }

    # Executar valida√ß√£o
    validation_results = validator.validate_optimization_results(
        sample_optimization_results,
        validation_method="walk_forward"
    )

    # Gerar relat√≥rio
    report_path = validator.generate_validation_report(
        validation_results,
        "../output/validation_report.md"
    )

    print(f"‚úÖ Relat√≥rio de valida√ß√£o criado: {report_path}")