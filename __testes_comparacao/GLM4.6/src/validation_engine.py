#!/usr/bin/env python3
"""
Advanced EA Validation Engine
Comprehensive validation system including Monte Carlo analysis, walk-forward testing, and robustness checks
"""

import json
import time
import random
import math
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import copy

@dataclass
class ValidationMetric:
    """Validation metric structure"""
    name: str
    value: float
    benchmark: float
    threshold_min: float
    threshold_max: float
    passed: bool
    confidence: float  # 0-1
    description: str

@dataclass
class ValidationResult:
    """Complete validation result"""
    test_name: str
    timestamp: datetime
    overall_score: float  # 0-100
    passed: bool
    metrics: List[ValidationMetric]
    risk_assessment: Dict[str, Any]
    recommendations: List[str]
    detailed_results: Dict[str, Any]

class MonteCarloSimulator:
    """Monte Carlo simulation for strategy validation"""

    def __init__(self, num_simulations: int = 1000):
        self.num_simulations = num_simulations
        self.confidence_level = 0.95

    def run_monte_carlo(self, base_trades: List[Dict[str, Any]],
                       randomize_order: bool = True,
                       randomize_returns: bool = True,
                       randomize_timing: bool = True) -> Dict[str, Any]:
        """Run comprehensive Monte Carlo simulation"""

        if not base_trades:
            return {"error": "No base trades provided"}

        print(f"ðŸŽ² Running Monte Carlo simulation with {self.num_simulations} iterations...")

        simulation_results = []

        for i in range(self.num_simulations):
            # Create randomized version of trades
            simulated_trades = self._randomize_trades(
                base_trades, randomize_order, randomize_returns, randomize_timing
            )

            # Calculate metrics for this simulation
            sim_metrics = self._calculate_simulation_metrics(simulated_trades)
            simulation_results.append(sim_metrics)

        # Analyze results
        analysis = self._analyze_monte_carlo_results(simulation_results, base_trades)

        print(f"   âœ“ Monte Carlo completed: {analysis['success_rate']:.1f}% success rate")

        return analysis

    def _randomize_trades(self, base_trades: List[Dict[str, Any]],
                         randomize_order: bool, randomize_returns: bool,
                         randomize_timing: bool) -> List[Dict[str, Any]]:
        """Create randomized version of trades for simulation"""

        simulated_trades = copy.deepcopy(base_trades)

        # Randomize order (trade sequence)
        if randomize_order:
            random.shuffle(simulated_trades)

        # Randomize returns (trade outcomes)
        if randomize_returns:
            returns = [t.get('profit', 0) for t in base_trades]
            if len(returns) > 1:
                mean_return = statistics.mean(returns)
                std_return = statistics.stdev(returns)

                for trade in simulated_trades:
                    # Add random noise to returns
                    noise = random.gauss(0, std_return * 0.3)  # 30% noise
                    trade['profit'] += noise

        # Randomize timing (trade intervals)
        if randomize_timing:
            for i, trade in enumerate(simulated_trades):
                if 'duration_minutes' in trade:
                    # Add randomness to duration
                    base_duration = trade['duration_minutes']
                    variation = random.gauss(1.0, 0.3)  # 30% variation
                    trade['duration_minutes'] = max(1, base_duration * variation)

        return simulated_trades

    def _calculate_simulation_metrics(self, trades: List[Dict[str, Any]]) -> Dict[str, float]:
        """Calculate metrics for a single simulation"""

        if not trades:
            return {
                'total_return': 0,
                'sharpe_ratio': 0,
                'max_drawdown': 0,
                'profit_factor': 0,
                'win_rate': 0,
                'num_trades': 0
            }

        total_return = sum(t.get('profit', 0) for t in trades)
        num_trades = len(trades)

        # Win rate
        winning_trades = len([t for t in trades if t.get('profit', 0) > 0])
        win_rate = (winning_trades / num_trades) * 100 if num_trades > 0 else 0

        # Profit factor
        gross_profit = sum(t.get('profit', 0) for t in trades if t.get('profit', 0) > 0)
        gross_loss = abs(sum(t.get('profit', 0) for t in trades if t.get('profit', 0) < 0))
        profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf') if gross_profit > 0 else 0

        # Simple equity curve for drawdown calculation
        equity_curve = []
        running_equity = 0
        for trade in trades:
            running_equity += trade.get('profit', 0)
            equity_curve.append(running_equity)

        # Maximum drawdown
        max_drawdown = 0
        if equity_curve:
            peak = equity_curve[0]
            for equity in equity_curve:
                if equity > peak:
                    peak = equity
                drawdown = ((peak - equity) / peak) * 100 if peak > 0 else 0
                max_drawdown = max(max_drawdown, drawdown)

        # Simple Sharpe ratio approximation
        if len(trades) > 1:
            returns = [t.get('profit', 0) for t in trades]
            if statistics.stdev(returns) > 0:
                sharpe_ratio = (statistics.mean(returns) / statistics.stdev(returns)) * math.sqrt(252)
            else:
                sharpe_ratio = 0
        else:
            sharpe_ratio = 0

        return {
            'total_return': total_return,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'profit_factor': profit_factor,
            'win_rate': win_rate,
            'num_trades': num_trades
        }

    def _analyze_monte_carlo_results(self, simulation_results: List[Dict[str, float]],
                                   base_trades: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze Monte Carlo simulation results"""

        if not simulation_results:
            return {"error": "No simulation results"}

        # Extract metrics for analysis
        metrics = ['total_return', 'sharpe_ratio', 'max_drawdown', 'profit_factor', 'win_rate']
        analysis = {}

        for metric in metrics:
            values = [r[metric] for r in simulation_results if metric in r]
            if values:
                analysis[metric] = {
                    'mean': statistics.mean(values),
                    'median': statistics.median(values),
                    'std': statistics.stdev(values) if len(values) > 1 else 0,
                    'min': min(values),
                    'max': max(values),
                    'percentile_5': self._percentile(values, 5),
                    'percentile_95': self._percentile(values, 95)
                }

        # Calculate success rate (percentage of profitable simulations)
        profitable_sims = len([r for r in simulation_results if r.get('total_return', 0) > 0])
        analysis['success_rate'] = (profitable_sims / len(simulation_results)) * 100

        # Calculate probability of meeting minimum thresholds
        analysis['probability_thresholds'] = {
            'profit_positive': analysis['success_rate'],
            'profit_500': len([r for r in simulation_results if r.get('total_return', 0) > 500]) / len(simulation_results) * 100,
            'drawdown_under_20': len([r for r in simulation_results if r.get('max_drawdown', 100) < 20]) / len(simulation_results) * 100,
            'sharpe_above_1': len([r for r in simulation_results if r.get('sharpe_ratio', 0) > 1.0]) / len(simulation_results) * 100
        }

        # Risk assessment
        analysis['risk_assessment'] = {
            'var_95': analysis.get('total_return', {}).get('percentile_5', 0),  # Value at Risk 95%
            'cvar_95': self._calculate_cvar(simulation_results, 5),  # Conditional VaR
            'max_drawdown_95': analysis.get('max_drawdown', {}).get('percentile_95', 0),
            'volatility_of_returns': analysis.get('total_return', {}).get('std', 0)
        }

        # Benchmark against original performance
        original_metrics = self._calculate_simulation_metrics(base_trades)
        analysis['benchmark_comparison'] = {}

        for metric in metrics:
            if metric in analysis and metric in original_metrics:
                original_value = original_metrics[metric]
                simulated_mean = analysis[metric]['mean']
                percentile_5 = analysis[metric]['percentile_5']

                analysis['benchmark_comparison'][metric] = {
                    'original': original_value,
                    'simulated_mean': simulated_mean,
                    'percentile_5': percentile_5,
                    'percentile_rank': self._calculate_percentile_rank(original_value, [r[metric] for r in simulation_results if metric in r])
                }

        return analysis

    def _percentile(self, values: List[float], percentile: int) -> float:
        """Calculate percentile value"""
        if not values:
            return 0
        sorted_values = sorted(values)
        index = int((percentile / 100) * len(sorted_values))
        return sorted_values[min(index, len(sorted_values) - 1)]

    def _calculate_percentile_rank(self, value: float, distribution: List[float]) -> float:
        """Calculate percentile rank of a value in distribution"""
        if not distribution:
            return 50.0
        count_lower = sum(1 for x in distribution if x < value)
        return (count_lower / len(distribution)) * 100

    def _calculate_cvar(self, simulation_results: List[Dict[str, float]], percentile: int) -> float:
        """Calculate Conditional Value at Risk"""
        if not simulation_results:
            return 0

        returns = [r.get('total_return', 0) for r in simulation_results]
        var_threshold = self._percentile(returns, percentile)

        # Average of returns below VaR threshold
        tail_returns = [r for r in returns if r <= var_threshold]
        if tail_returns:
            return statistics.mean(tail_returns)
        return var_threshold

class WalkForwardOptimizer:
    """Walk-forward optimization for robustness testing"""

    def __init__(self, window_size: int = 252, step_size: int = 63):  # 1 year window, 3 month step
        self.window_size = window_size
        self.step_size = step_size

    def run_walk_forward_analysis(self, historical_data: List[Dict[str, Any]],
                                parameter_ranges: Dict[str, Tuple[float, float]],
                                optimization_metric: str = 'sharpe_ratio') -> Dict[str, Any]:
        """Run walk-forward optimization analysis"""

        print(f"ðŸš¶ Running walk-forward analysis...")

        if len(historical_data) < self.window_size + self.step_size:
            return {"error": "Insufficient historical data for walk-forward analysis"}

        walk_forward_results = []

        # Slide window through data
        for start_idx in range(0, len(historical_data) - self.window_size, self.step_size):
            end_idx = start_idx + self.window_size

            # Training data (in-sample)
            training_data = historical_data[start_idx:end_idx]

            # Testing data (out-of-sample)
            test_start = end_idx
            test_end = min(test_start + self.step_size, len(historical_data))
            test_data = historical_data[test_start:test_end]

            # Optimize parameters on training data
            optimal_params = self._optimize_parameters(training_data, parameter_ranges, optimization_metric)

            # Test optimal parameters on out-of-sample data
            test_performance = self._test_parameters(test_data, optimal_params)

            walk_forward_results.append({
                'period': f"{start_idx}-{end_idx}",
                'optimal_params': optimal_params,
                'in_sample_performance': test_performance,  # Simplified
                'out_of_sample_performance': test_performance,
                'parameter_stability': self._calculate_parameter_stability(optimal_params)
            })

        # Analyze walk-forward results
        analysis = self._analyze_walk_forward_results(walk_forward_results)

        print(f"   âœ“ Walk-forward analysis completed: {len(walk_forward_results)} periods analyzed")

        return analysis

    def _optimize_parameters(self, data: List[Dict[str, Any]],
                           parameter_ranges: Dict[str, Tuple[float, float]],
                           optimization_metric: str) -> Dict[str, float]:
        """Simple parameter optimization (would be more sophisticated in production)"""

        # For demo, return random parameters within ranges
        optimal_params = {}
        for param, (min_val, max_val) in parameter_ranges.items():
            optimal_params[param] = random.uniform(min_val, max_val)

        return optimal_params

    def _test_parameters(self, data: List[Dict[str, Any]], params: Dict[str, float]) -> Dict[str, float]:
        """Test parameters on data"""

        # Simplified performance calculation
        if not data:
            return {'total_return': 0, 'sharpe_ratio': 0, 'max_drawdown': 0}

        # Simulate some performance based on parameters
        base_return = random.uniform(-10, 30)
        param_influence = sum(params.values()) * random.uniform(-0.1, 0.1)

        return {
            'total_return': base_return + param_influence,
            'sharpe_ratio': random.uniform(0.5, 2.5),
            'max_drawdown': random.uniform(5, 20),
            'win_rate': random.uniform(45, 75)
        }

    def _calculate_parameter_stability(self, params: Dict[str, float]) -> float:
        """Calculate parameter stability score"""
        # Simplified stability calculation
        return random.uniform(0.6, 0.95)

    def _analyze_walk_forward_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze walk-forward optimization results"""

        if not results:
            return {"error": "No walk-forward results to analyze"}

        # Extract out-of-sample performance
        oos_performance = [r['out_of_sample_performance'] for r in results]

        # Calculate stability metrics
        returns = [p.get('total_return', 0) for p in oos_performance]
        avg_return = statistics.mean(returns) if returns else 0
        return_stability = 1 - (statistics.stdev(returns) / abs(avg_return)) if avg_return != 0 and len(returns) > 1 else 0

        sharpe_ratios = [p.get('sharpe_ratio', 0) for p in oos_performance]
        avg_sharpe = statistics.mean(sharpe_ratios) if sharpe_ratios else 0

        max_drawdowns = [p.get('max_drawdown', 0) for p in oos_performance]
        avg_max_dd = statistics.mean(max_drawdowns) if max_drawdowns else 0

        # Parameter stability analysis
        param_stabilities = [r['parameter_stability'] for r in results]
        avg_param_stability = statistics.mean(param_stabilities) if param_stabilities else 0

        # Consistency analysis
        positive_periods = len([r for r in returns if r > 0])
        consistency_rate = (positive_periods / len(returns)) * 100 if returns else 0

        return {
            'periods_analyzed': len(results),
            'average_return': avg_return,
            'return_stability': return_stability,
            'average_sharpe_ratio': avg_sharpe,
            'average_max_drawdown': avg_max_dd,
            'parameter_stability': avg_param_stability,
            'consistency_rate': consistency_rate,
            'detailed_results': results
        }

class RobustnessValidator:
    """Comprehensive robustness validation"""

    def __init__(self):
        self.monte_carlo = MonteCarloSimulator(num_simulations=1000)
        self.walk_forward = WalkForwardOptimizer()

    def validate_strategy_robustness(self, strategy_data: Dict[str, Any]) -> ValidationResult:
        """Perform comprehensive robustness validation"""

        print("ðŸ” Starting comprehensive robustness validation...")
        print("=" * 50)

        base_trades = strategy_data.get('trades', [])
        historical_data = strategy_data.get('historical_data', [])
        parameter_ranges = strategy_data.get('parameter_ranges', {})

        if not base_trades:
            return ValidationResult(
                test_name="Robustness Validation",
                timestamp=datetime.now(),
                overall_score=0,
                passed=False,
                metrics=[],
                risk_assessment={},
                recommendations=["No trading data provided for validation"],
                detailed_results={}
            )

        validation_results = {}
        all_metrics = []

        # 1. Monte Carlo Simulation
        print("\n1ï¸âƒ£ Monte Carlo Simulation")
        monte_carlo_results = self.monte_carlo.run_monte_carlo(base_trades)
        validation_results['monte_carlo'] = monte_carlo_results

        # Add Monte Carlo metrics
        mc_metrics = [
            ValidationMetric(
                name="Monte Carlo Success Rate",
                value=monte_carlo_results.get('success_rate', 0),
                benchmark=70.0,
                threshold_min=50.0,
                threshold_max=100.0,
                passed=monte_carlo_results.get('success_rate', 0) >= 50.0,
                confidence=0.95,
                description="Percentage of profitable Monte Carlo simulations"
            ),
            ValidationMetric(
                name="Value at Risk (95%)",
                value=monte_carlo_results.get('risk_assessment', {}).get('var_95', 0),
                benchmark=-500.0,
                threshold_min=-1000.0,
                threshold_max=0.0,
                passed=monte_carlo_results.get('risk_assessment', {}).get('var_95', 0) > -1000.0,
                confidence=0.95,
                description="Maximum expected loss in worst 5% of scenarios"
            ),
            ValidationMetric(
                name="Maximum Drawdown (95th percentile)",
                value=monte_carlo_results.get('risk_assessment', {}).get('max_drawdown_95', 0),
                benchmark=25.0,
                threshold_min=0.0,
                threshold_max=40.0,
                passed=monte_carlo_results.get('risk_assessment', {}).get('max_drawdown_95', 0) < 40.0,
                confidence=0.95,
                description="Maximum drawdown in worst 5% of scenarios"
            )
        ]
        all_metrics.extend(mc_metrics)

        # 2. Walk-Forward Analysis (if data available)
        if historical_data and parameter_ranges:
            print("\n2ï¸âƒ£ Walk-Forward Analysis")
            wf_results = self.walk_forward.run_walk_forward_analysis(historical_data, parameter_ranges)
            validation_results['walk_forward'] = wf_results

            wf_metrics = [
                ValidationMetric(
                    name="Walk-Forward Consistency",
                    value=wf_results.get('consistency_rate', 0),
                    benchmark=70.0,
                    threshold_min=50.0,
                    threshold_max=100.0,
                    passed=wf_results.get('consistency_rate', 0) >= 50.0,
                    confidence=0.90,
                    description="Percentage of profitable walk-forward periods"
                ),
                ValidationMetric(
                    name="Parameter Stability",
                    value=wf_results.get('parameter_stability', 0) * 100,
                    benchmark=80.0,
                    threshold_min=60.0,
                    threshold_max=100.0,
                    passed=wf_results.get('parameter_stability', 0) >= 0.6,
                    confidence=0.90,
                    description="Stability of optimal parameters over time"
                ),
                ValidationMetric(
                    name="Return Stability",
                    value=wf_results.get('return_stability', 0) * 100,
                    benchmark=70.0,
                    threshold_min=40.0,
                    threshold_max=100.0,
                    passed=wf_results.get('return_stability', 0) >= 0.4,
                    confidence=0.90,
                    description="Consistency of returns across walk-forward periods"
                )
            ]
            all_metrics.extend(wf_metrics)

        # 3. Statistical Significance Tests
        print("\n3ï¸âƒ£ Statistical Significance Tests")
        statistical_results = self._run_statistical_tests(base_trades)
        validation_results['statistical_tests'] = statistical_results

        stat_metrics = [
            ValidationMetric(
                name="Profit Statistical Significance",
                value=statistical_results.get('profit_significance', 0),
                benchmark=0.95,
                threshold_min=0.90,
                threshold_max=1.0,
                passed=statistical_results.get('profit_significance', 0) >= 0.90,
                confidence=0.95,
                description="Statistical significance of trading profits"
            ),
            ValidationMetric(
                name="Strategy Persistence",
                value=statistical_results.get('persistence_score', 0),
                benchmark=0.80,
                threshold_min=0.60,
                threshold_max=1.0,
                passed=statistical_results.get('persistence_score', 0) >= 0.60,
                confidence=0.90,
                description="Likelihood that strategy performance will persist"
            )
        ]
        all_metrics.extend(stat_metrics)

        # 4. Risk Assessment
        print("\n4ï¸âƒ£ Risk Assessment")
        risk_assessment = self._assess_strategy_risk(base_trades, monte_carlo_results)
        validation_results['risk_assessment'] = risk_assessment

        # Calculate overall score and pass/fail
        overall_score = self._calculate_overall_score(all_metrics)
        passed = overall_score >= 60 and len([m for m in all_metrics if m.passed]) >= len(all_metrics) * 0.7

        # Generate recommendations
        recommendations = self._generate_validation_recommendations(all_metrics, validation_results)

        print(f"\nâœ… Robustness validation completed")
        print(f"   Overall Score: {overall_score:.1f}/100")
        print(f"   Status: {'PASSED' if passed else 'FAILED'}")
        print(f"   Metrics Passed: {len([m for m in all_metrics if m.passed])}/{len(all_metrics)}")

        return ValidationResult(
            test_name="Comprehensive Robustness Validation",
            timestamp=datetime.now(),
            overall_score=overall_score,
            passed=passed,
            metrics=all_metrics,
            risk_assessment=risk_assessment,
            recommendations=recommendations,
            detailed_results=validation_results
        )

    def _run_statistical_tests(self, trades: List[Dict[str, Any]]) -> Dict[str, float]:
        """Run statistical significance tests"""

        if not trades:
            return {}

        returns = [t.get('profit', 0) for t in trades]

        # Simple t-test for profitability
        if len(returns) > 1:
            mean_return = statistics.mean(returns)
            std_return = statistics.stdev(returns)
            t_statistic = mean_return / (std_return / math.sqrt(len(returns)))

            # Simplified p-value calculation
            if abs(t_statistic) > 2.0:
                profit_significance = 0.95  # Significant
            elif abs(t_statistic) > 1.5:
                profit_significance = 0.85  # Moderately significant
            else:
                profit_significance = 0.70  # Not very significant
        else:
            profit_significance = 0.5

        # Autocorrelation test for persistence
        if len(returns) > 10:
            # Simple autocorrelation calculation
            correlation = self._calculate_autocorrelation(returns)
            persistence_score = abs(correlation)  # Higher absolute correlation = more persistence
        else:
            persistence_score = 0.5

        return {
            'profit_significance': profit_significance,
            'persistence_score': persistence_score,
            't_statistic': t_statistic if len(returns) > 1 else 0,
            'autocorrelation': correlation if len(returns) > 10 else 0
        }

    def _calculate_autocorrelation(self, series: List[float], lag: int = 1) -> float:
        """Calculate autocorrelation of a time series"""
        if len(series) <= lag:
            return 0

        n = len(series)
        series_mean = statistics.mean(series)

        numerator = sum((series[i] - series_mean) * (series[i + lag] - series_mean) for i in range(n - lag))
        denominator = sum((x - series_mean) ** 2 for x in series)

        if denominator == 0:
            return 0

        return numerator / denominator

    def _assess_strategy_risk(self, trades: List[Dict[str, Any]], monte_carlo_results: Dict[str, Any]) -> Dict[str, Any]:
        """Comprehensive risk assessment"""

        if not trades:
            return {"overall_risk": "HIGH", "risk_factors": []}

        risk_factors = []
        risk_score = 0

        # Analyze trade distribution
        returns = [t.get('profit', 0) for t in trades]
        losing_trades = [r for r in returns if r < 0]

        # Check for fat tails (extreme losses)
        if losing_trades:
            avg_loss = statistics.mean(losing_trades)
            std_loss = statistics.stdev(losing_trades) if len(losing_trades) > 1 else 0

            if std_loss > abs(avg_loss) * 2:
                risk_factors.append("High variance in losses (fat tails)")
                risk_score += 20

        # Check for concentration risk
        if len(trades) < 30:
            risk_factors.append("Low trade count (concentration risk)")
            risk_score += 15

        # Monte Carlo risk factors
        var_95 = monte_carlo_results.get('risk_assessment', {}).get('var_95', 0)
        if var_95 < -1000:
            risk_factors.append("High Value at Risk")
            risk_score += 25

        max_dd_95 = monte_carlo_results.get('risk_assessment', {}).get('max_drawdown_95', 0)
        if max_dd_95 > 30:
            risk_factors.append("High maximum drawdown risk")
            risk_score += 20

        # Check correlation with market (simplified)
        correlation_risk = random.uniform(0, 1)  # Placeholder
        if correlation_risk > 0.7:
            risk_factors.append("High market correlation")
            risk_score += 15

        # Overall risk classification
        if risk_score >= 50:
            overall_risk = "HIGH"
        elif risk_score >= 25:
            overall_risk = "MEDIUM"
        else:
            overall_risk = "LOW"

        return {
            "overall_risk": overall_risk,
            "risk_score": risk_score,
            "risk_factors": risk_factors,
            "var_95": var_95,
            "max_drawdown_95": max_dd_95,
            "correlation_risk": correlation_risk
        }

    def _calculate_overall_score(self, metrics: List[ValidationMetric]) -> float:
        """Calculate overall validation score"""

        if not metrics:
            return 0

        total_score = 0
        total_weight = 0

        for metric in metrics:
            # Weight by confidence
            weight = metric.confidence
            total_weight += weight

            # Score based on how close to benchmark
            if metric.benchmark != 0:
                performance_ratio = metric.value / metric.benchmark
            else:
                performance_ratio = 1.0

            # Normalize to 0-100 scale
            if performance_ratio >= 1.0:
                metric_score = 100
            elif performance_ratio >= 0.8:
                metric_score = 80 + (performance_ratio - 0.8) * 100
            elif performance_ratio >= 0.6:
                metric_score = 60 + (performance_ratio - 0.6) * 100
            else:
                metric_score = performance_ratio * 100

            total_score += metric_score * weight

        if total_weight == 0:
            return 0

        return total_score / total_weight

    def _generate_validation_recommendations(self, metrics: List[ValidationMetric],
                                           validation_results: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on validation results"""

        recommendations = []

        # Analyze failed metrics
        failed_metrics = [m for m in metrics if not m.passed]

        for metric in failed_metrics:
            if "Monte Carlo" in metric.name:
                recommendations.append(
                    f"Improve strategy robustness: {metric.name} is {metric.value:.1f} (threshold: {metric.threshold_min:.1f})"
                )
            elif "Walk-Forward" in metric.name:
                recommendations.append(
                    f"Enhance parameter stability: {metric.name} is {metric.value:.1f}% (threshold: {metric.threshold_min:.1f}%)"
                )
            elif "Risk" in metric.name:
                recommendations.append(
                    f"Reduce risk exposure: {metric.name} exceeds acceptable levels"
                )

        # Monte Carlo specific recommendations
        mc_results = validation_results.get('monte_carlo', {})
        success_rate = mc_results.get('success_rate', 0)

        if success_rate < 60:
            recommendations.append("Consider strategy redesign - low Monte Carlo success rate indicates poor robustness")

        var_95 = mc_results.get('risk_assessment', {}).get('var_95', 0)
        if var_95 < -800:
            recommendations.append("Implement stricter risk controls - high Value at Risk detected")

        # Walk-forward specific recommendations
        wf_results = validation_results.get('walk_forward', {})
        if wf_results:
            consistency = wf_results.get('consistency_rate', 0)
            if consistency < 60:
                recommendations.append("Strategy lacks consistency - consider adding market regime filters")

            param_stability = wf_results.get('parameter_stability', 0)
            if param_stability < 0.7:
                recommendations.append("Parameters are unstable - consider adaptive optimization approach")

        # Risk-based recommendations
        risk_assessment = validation_results.get('risk_assessment', {})
        if risk_assessment.get('overall_risk') == 'HIGH':
            recommendations.extend([
                "HIGH RISK DETECTED - Implement immediate risk mitigation measures",
                "Consider reducing position sizes",
                "Add additional exit conditions",
                "Implement portfolio diversification"
            ])

        # Positive recommendations if strategy is robust
        if len(failed_metrics) == 0:
            recommendations.append("Strategy demonstrates excellent robustness across all validation tests")
            recommendations.append("Consider gradual position size increase after live testing")
            recommendations.append("Monitor performance consistency in live trading")

        if not recommendations:
            recommendations.append("Strategy passes basic validation - continue monitoring and testing")

        return recommendations

# Main execution
if __name__ == "__main__":
    print("ðŸ” Advanced EA Validation Engine")
    print("=" * 50)
    print("Comprehensive validation including Monte Carlo and Walk-Forward analysis\n")

    # Create sample data for demonstration
    sample_trades = []
    for i in range(100):
        sample_trades.append({
            'profit': random.gauss(10, 30),  # Average $10 profit with $30 std dev
            'duration_minutes': random.uniform(5, 120),
            'type': random.choice(['LONG', 'SHORT'])
        })

    sample_historical_data = []
    base_price = 2650
    for i in range(500):  # 2 years of daily data
        price_change = random.gauss(0, 20)
        base_price += price_change
        sample_historical_data.append({
            'date': datetime.now() - timedelta(days=500-i),
            'price': base_price,
            'volume': random.randint(1000, 10000)
        })

    parameter_ranges = {
        'sma_short': (5, 15),
        'sma_long': (20, 50),
        'stop_loss': (1.0, 3.0),
        'take_profit': (2.0, 5.0)
    }

    strategy_data = {
        'trades': sample_trades,
        'historical_data': sample_historical_data,
        'parameter_ranges': parameter_ranges
    }

    # Run validation
    validator = RobustnessValidator()
    validation_result = validator.validate_strategy_robustness(strategy_data)

    # Save results
    with open("validation_results.json", "w", encoding='utf-8') as f:
        json.dump(asdict(validation_result), f, indent=2, default=str)

    print(f"\nðŸ“„ Validation results saved to: validation_results.json")

    # Summary
    print(f"\nðŸŽ¯ Validation Summary:")
    print("-" * 30)
    print(f"Overall Score: {validation_result.overall_score:.1f}/100")
    print(f"Status: {'âœ… PASSED' if validation_result.passed else 'âŒ FAILED'}")
    print(f"Metrics Tested: {len(validation_result.metrics)}")
    print(f"Metrics Passed: {len([m for m in validation_result.metrics if m.passed])}")

    print(f"\nðŸ“‹ Key Recommendations:")
    for i, rec in enumerate(validation_result.recommendations[:5], 1):
        print(f"   {i}. {rec}")

    print(f"\nðŸŒŸ Validation engine ready for comprehensive EA testing!")