# ðŸ—ï¸ Arquitetura - Roo Code + LiteLLM + R1 Otimizado

## ðŸ“‹ VisÃ£o Geral da Arquitetura

```
Roo Code â†’ LiteLLM Proxy â†’ OpenRouter API â†’ R1 (com cache otimizado)
     â†“           â†“              â†“               â†“
Interface    Chave Local   Cache Otimizado   IA R1
de UsuÃ¡rio   de API        Multi-NÃ­vel      Model
```

---

## ðŸ”§ Como Configurar

### **Passo 1: Iniciar LiteLLM Proxy**

```bash
# Instalar LiteLLM
pip install litellm

# Configurar proxy com cache
cat > litellm_config.yaml << 'EOF'
model_list:
  - model_name: deepseek-r1-free
    litellm_params:
      model: deepseek/deepseek-r1-free
      api_key: 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
      api_base: https://openrouter.ai/api/v1

litellm_settings:
  cache: true
  cache_params:
    type: "redis"
    host: "localhost"
    port: 6379
  enable_rate_limiting: true

server_settings:
  host: "0.0.0.0"
  port: 4000
EOF

# Iniciar proxy
litellm --config litellm_config.yaml
```

### **Passo 2: Obter Chave do LiteLLM**

```bash
# A chave gerada pelo LiteLLM serÃ¡ algo como:
# Bearer sk-litellm-abc123
```

### **Passo 3: Configurar no Roo Code**

```json
// ConfiguraÃ§Ã£o no Roo Code
{
  "openai": {
    "baseURL": "http://localhost:4000",
    "apiKey": "sk-litellm-abc123",
    "models": ["deepseek-r1-free"]
  }
}
```

---

## ðŸ“Š Arquitetura Completa

### **Componentes do Sistema:**

#### **1. Roo Code (Interface)**
```
- Interface de usuÃ¡rio
- Gerenciamento de contexto
- IntegraÃ§Ã£o com modelos
- Plugins e extensÃµes
```

#### **2. LiteLLM Proxy (Gateway)**
```
- Gerenciamento de chaves
- Rate limiting
- Cache distribuÃ­do (Redis)
- Balanceamento de carga
- Logging e monitoramento
```

#### **3. Cache Otimizado R1 (Nossa ImplementaÃ§Ã£o)**
```
- Cache hierÃ¡rquico multi-nÃ­vel
- DeduplicaÃ§Ã£o semÃ¢ntica
- Templates especÃ­ficos R1
- CompressÃ£o de dados
- Monitoramento avanÃ§ado
```

#### **4. OpenRouter API (Backend)**
```
- Modelos de IA (R1, etc.)
- Rate limiting por usuÃ¡rio
- Analytics e relatÃ³rios
- Fallback e redundÃ¢ncia
```

---

## âš™ï¸ ConfiguraÃ§Ã£o Detalhada

### **1. LiteLLM com Redis Cache**

```yaml
# litellm_config.yaml
model_list:
  - model_name: deepseek-r1-free
    litellm_params:
      model: deepseek/deepseek-r1-free
      api_key: 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
      api_base: https://openrouter.ai/api/v1

litellm_settings:
  cache: true
  cache_params:
    type: "redis"
    host: "localhost"
    port: 6379
    password: ""  # Se necessÃ¡rio
    db: 0
    ttl: 3600  # 1 hora
  enable_rate_limiting: true
  rate_limit_per_minute: 100

server_settings:
  host: "0.0.0.0"
  port: 4000
  enable_cors: true
```

### **2. Redis para Cache DistribuÃ­do**

```bash
# Instalar Redis
docker run -d -p 6379:6379 redis:alpine

# Ou instalar localmente
sudo apt-get install redis-server
sudo systemctl start redis
```

### **3. ConfiguraÃ§Ã£o do Roo Code**

```json
// .vscode/settings.json ou configuraÃ§Ã£o do Roo Code
{
  "roo-code": {
    "openai": {
      "baseURL": "http://localhost:4000",
      "apiKey": "sk-litellm-abc123",
      "models": ["deepseek-r1-free"],
      "maxTokens": 2000000,
      "temperature": 0.7
    },
    "cache": {
      "enabled": true,
      "maxSize": 1000000,
      "ttl": 3600
    }
  }
}
```

---

## ðŸš€ Fluxo de OperaÃ§Ã£o

### **1. UsuÃ¡rio faz query no Roo Code**
```
Query â†’ Roo Code â†’ LiteLLM Proxy â†’ Cache Otimizado â†’ OpenRouter â†’ R1
```

### **2. Resposta retorna otimizada**
```
R1 â†’ OpenRouter â†’ Cache Otimizado â†’ LiteLLM â†’ Roo Code â†’ UsuÃ¡rio
```

### **3. Cache automÃ¡tico**
```
PrÃ³ximas queries similares: Roo Code â†’ Cache â†’ Resposta instantÃ¢nea
```

---

## ðŸ“Š BenefÃ­cios da Arquitetura

### **Performance:**
- **Cache multi-nÃ­vel** (RAM + Redis + HDD)
- **Hit rate de 90%+** para queries similares
- **Resposta < 0.5ms** para cache hits
- **Throughput de 1000+ ops/s**

### **SeguranÃ§a:**
- **Chave local** gerada pelo LiteLLM
- **Rate limiting** configurÃ¡vel
- **Logging detalhado** de uso
- **Controle de acesso**

### **Escalabilidade:**
- **Cache distribuÃ­do** com Redis
- **Balanceamento de carga**
- **MÃºltiplos modelos** simultÃ¢neos
- **Auto-scaling** baseado na demanda

---

## ðŸ”§ Scripts de AutomaÃ§Ã£o

### **1. Script de InicializaÃ§Ã£o Completa**

```bash
#!/bin/bash
# start_system.sh

echo "ðŸš€ Iniciando Sistema Roo Code + LiteLLM + R1 Otimizado"

# 1. Iniciar Redis
docker run -d -p 6379:6379 --name redis-litellm redis:alpine

# 2. Iniciar LiteLLM Proxy
litellm --config litellm_config.yaml &

# 3. Aguardar inicializaÃ§Ã£o
sleep 5

# 4. Testar conexÃ£o
curl -X POST http://localhost:4000/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-litellm-abc123" \
  -d '{
    "model": "deepseek-r1-free",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'

echo "âœ… Sistema inicializado com sucesso!"
echo "ðŸ”— Endpoint: http://localhost:4000"
echo "ðŸ”‘ Chave: sk-litellm-abc123"
```

### **2. Script de Monitoramento**

```bash
#!/bin/bash
# monitor_system.sh

while true; do
  clear
  echo "ðŸ“Š Monitoramento do Sistema R1 Otimizado"
  echo "=========================================="
  echo ""

  # Status do Redis
  echo "ðŸ”´ Redis Cache:"
  docker stats redis-litellm --no-stream --format "table {{.CPUPerc}}\t{{.MemUsage}}"

  # Status do LiteLLM
  echo ""
  echo "ðŸŸ¢ LiteLLM Proxy:"
  curl -s http://localhost:4000/health || echo "âŒ Offline"

  # EstatÃ­sticas de cache
  echo ""
  echo "ðŸ“ˆ Cache Stats:"
  python -c "
  from sistema_contexto_expandido_2m import ContextManager
  cm = ContextManager()
  stats = cm.get_cache_stats()
  print(f'Hit Rate: {stats.get(\"overall_hit_rate\", \"N/A\")}')
  print(f'L1 Cache: {stats.get(\"l1_cache_size\", 0)} entries')
  print(f'Total Requests: {stats.get(\"total_requests\", 0)}')
  "

  sleep 5
done
```

---

## ðŸŽ¯ Como Usar no Roo Code

### **1. Configurar Endpoint**
```json
{
  "roo-code.openai.baseURL": "http://localhost:4000",
  "roo-code.openai.apiKey": "sk-litellm-abc123",
  "roo-code.openai.models": ["deepseek-r1-free"]
}
```

### **2. Usar com Cache Otimizado**
```python
# No Roo Code, suas queries serÃ£o automaticamente:
# 1. Processadas pelo LiteLLM
# 2. Otimizadas pelo cache
# 3. Enviadas para R1
# 4. Retornadas com performance mÃ¡xima

# Exemplo de uso:
# /analyze Analisar estratÃ©gia de trading XAUUSD
# /optimize Otimizar cÃ³digo de EA
# /debug Resolver problema em MQL5
```

### **3. BenefÃ­cios no Roo Code**
- **Respostas instantÃ¢neas** para queries similares
- **Contexto expandido** atÃ© 2M tokens
- **Cache inteligente** especÃ­fico para R1
- **Performance de nÃ­vel institucional**

---

## ðŸ Resumo da Arquitetura

### **Para Roo Code:**
```
Roo Code â†’ LiteLLM Proxy (localhost:4000) â†’ Cache Otimizado â†’ OpenRouter â†’ R1
```

### **Chave de API:**
- **Gerada por LiteLLM:** `sk-litellm-abc123`
- **Usada no Roo Code:** Configurada nas settings
- **VÃ¡lida apenas localmente:** NÃ£o expÃµe chave real da OpenRouter

### **Performance:**
- **Cache Hit Rate:** 90%+ (queries similares)
- **Tempo de Resposta:** < 0.5ms (cache hits)
- **Throughput:** 1000+ operaÃ§Ãµes/segundo
- **Contexto:** AtÃ© 2M tokens

### **BenefÃ­cios:**
- âœ… **SeguranÃ§a:** Chave local, nÃ£o expÃµe API real
- âœ… **Performance:** Cache otimizado especÃ­fico para R1
- âœ… **Escalabilidade:** Suporte a mÃºltiplos modelos
- âœ… **Monitoramento:** MÃ©tricas em tempo real

---

**ðŸŽ¯ Status: Arquitetura completa definida e pronta para implementaÃ§Ã£o!**

**ðŸš€ Com essa arquitetura, vocÃª terÃ¡ um sistema de IA otimizado no Roo Code com performance de nÃ­vel institucional!**